{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f16224",
   "metadata": {},
   "source": [
    "### 这个 notebook 用来读取文件，并测试一些方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb46bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 所有 Python 文件内容已写入 src.txt（共 11 个文件）\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_all_py_files(root_dir: str) -> list:\n",
    "    \"\"\"\n",
    "    获取指定目录及其子目录下的所有 .py 文件路径。\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): 起始目录路径。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含所有 .py 文件完整路径的列表。\n",
    "    \"\"\"\n",
    "    py_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".py\"):\n",
    "                py_files.append(os.path.join(dirpath, file))\n",
    "    return py_files\n",
    "\n",
    "def write_all_py_contents_to_output(py_files: list, output_path: str = \"src.txt\") -> None:\n",
    "    \"\"\"\n",
    "    将所有 .py 文件的内容写入一个文本文件中，并打印文件名作为分隔。\n",
    "\n",
    "    Args:\n",
    "        py_files (list): .py 文件路径列表。\n",
    "        output_path (str): 输出文件路径。\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        for path in py_files:\n",
    "            out_file.write(f\"{'=' * 80}\\n\")\n",
    "            out_file.write(f\"File: {path}\\n\")\n",
    "            out_file.write(f\"{'-' * 80}\\n\")\n",
    "            try:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    out_file.write(f.read())\n",
    "            except Exception as e:\n",
    "                out_file.write(f\"⚠️ Error reading {path}: {e}\\n\")\n",
    "            out_file.write(\"\\n\\n\")\n",
    "\n",
    "if True:\n",
    "    root_directory = \".\"  # 当前目录\n",
    "    all_py_files = get_all_py_files(root_directory)\n",
    "    write_all_py_contents_to_output(all_py_files)\n",
    "    print(f\"📄 所有 Python 文件内容已写入 src.txt（共 {len(all_py_files)} 个文件）\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3581e",
   "metadata": {},
   "source": [
    "### 检查 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194f4d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 10, 28, 2]\n",
      "open 5 ts_std\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import AlphaTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tok = AlphaTokenizer()\n",
    "\n",
    "expr = \"open 5 ts_std\"          # 相当于 ts_mean(close, 5)\n",
    "\n",
    "ids = tok.encode(expr)\n",
    "print(ids)        # ➜ [BOS]  对应 id,  ... , [SEP]\n",
    "\n",
    "print(tok.decode(ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ddfd39",
   "metadata": {},
   "source": [
    "### 1.测试 AlphaCombinationModel._compute_alpha_from_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70fb8b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha shape: (64861,)\n",
      "alpha sample: [2971.82 2971.85 2971.83 2971.81 2971.81]\n"
     ]
    }
   ],
   "source": [
    "from combination import AlphaCombinationModel\n",
    "from data import load_market_data\n",
    "\n",
    "df = load_market_data()\n",
    "model = AlphaCombinationModel()\n",
    "model.inject_data(df, target_col='target')\n",
    "\n",
    "# 表达式：close 的 100 秒均值\n",
    "expr = \"close 100 ts_mean\"\n",
    "alpha = model._compute_alpha_from_expr(expr)\n",
    "\n",
    "print(\"alpha shape:\", alpha.shape)\n",
    "print(\"alpha sample:\", alpha[~np.isnan(alpha)][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bc3e9",
   "metadata": {},
   "source": [
    "###  2. 测试 AlphaCombinationModel.add_alpha_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc603c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该因子的 IC 为： 0.028037411051210277\n",
      "当前池中因子数： 1\n"
     ]
    }
   ],
   "source": [
    "ic = model.add_alpha_expr(\"high low - 100 ts_max\")\n",
    "print(\"该因子的 IC 为：\", ic)\n",
    "print(\"当前池中因子数：\", len(model.alphas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba8fd7",
   "metadata": {},
   "source": [
    "### 3. 测试 AlphaTokenizer.encode / decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda7d163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1, 6, 10, 23, 2]\n",
      "Decoded expr: close 5 ts_mean\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import AlphaTokenizer\n",
    "\n",
    "tokenizer = AlphaTokenizer()\n",
    "\n",
    "expr = \"close 5 ts_mean\"\n",
    "ids = tokenizer.encode(expr)\n",
    "decoded = tokenizer.decode(ids)\n",
    "\n",
    "print(\"Token IDs:\", ids)\n",
    "print(\"Decoded expr:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1899091",
   "metadata": {},
   "source": [
    "### 4. 测试 AlphaGenerationEnv.reset / step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5018150f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始状态 token IDs: [1]\n",
      "新状态: [1, 3]\n",
      "Reward: 0.0 Done: False\n"
     ]
    }
   ],
   "source": [
    "from envs import AlphaGenerationEnv\n",
    "from combination import AlphaCombinationModel\n",
    "from tokenizer import AlphaTokenizer\n",
    "from data import load_market_data\n",
    "\n",
    "df = load_market_data()\n",
    "combo = AlphaCombinationModel()\n",
    "combo.inject_data(df, target_col='target')\n",
    "tokenizer = AlphaTokenizer()\n",
    "env = AlphaGenerationEnv(combo, tokenizer)\n",
    "\n",
    "obs = env.reset()\n",
    "print(\"初始状态 token IDs:\", obs)\n",
    "\n",
    "# 尝试一步动作（合法动作中任选一个）\n",
    "valid = env.valid_actions()\n",
    "action = valid[0]\n",
    "obs2, reward, done, info = env.step(action)\n",
    "print(\"新状态:\", obs2)\n",
    "print(\"Reward:\", reward, \"Done:\", done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1291c",
   "metadata": {},
   "source": [
    "### 5. 测试 PolicyNetwork / ValueNetwork 输出维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90c5b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy logits shape: torch.Size([1, 50])\n",
      "Value estimate shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from generator import PolicyNetwork, ValueNetwork\n",
    "\n",
    "vocab_size = 50\n",
    "seq_len = 6\n",
    "hidden_dim = 64\n",
    "device = \"cpu\"\n",
    "\n",
    "x = torch.randint(0, vocab_size, (1, seq_len))  # batch_size=1\n",
    "policy = PolicyNetwork(vocab_size, hidden_dim).to(device)\n",
    "value = ValueNetwork(vocab_size, hidden_dim).to(device)\n",
    "\n",
    "h0_p = policy.init_hidden(1, device)\n",
    "logits, _ = policy(x, h0_p)\n",
    "print(\"Policy logits shape:\", logits.shape)  # 应为 (1, vocab_size)\n",
    "\n",
    "h0_v = value.init_hidden(1, device)\n",
    "v, _ = value(x, h0_v)\n",
    "print(\"Value estimate shape:\", v.shape)      # 应为 (1,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b7e9d",
   "metadata": {},
   "source": [
    "### 6. 测试 RLAlphaGenerator._collect_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b861b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from utility import set_random_seed\n",
    "\n",
    "def reload_components():\n",
    "    \"\"\"\n",
    "    重新加载以下模块，以便在开发过程中即时生效：\n",
    "      - data.load_market_data\n",
    "      - tokenizer.AlphaTokenizer\n",
    "      - combination.AlphaCombinationModel\n",
    "      - envs.AlphaGenerationEnv\n",
    "      - generator.RLAlphaGenerator\n",
    "    \"\"\"\n",
    "    import data, tokenizer, combination, envs, generator\n",
    "\n",
    "    importlib.reload(data)\n",
    "    importlib.reload(tokenizer)\n",
    "    importlib.reload(combination)\n",
    "    importlib.reload(envs)\n",
    "    importlib.reload(generator)\n",
    "\n",
    "    # 重新绑定到本地名称（可选）\n",
    "    from data import load_market_data\n",
    "    from tokenizer import AlphaTokenizer\n",
    "    from combination import AlphaCombinationModel\n",
    "    from envs import AlphaGenerationEnv\n",
    "    from generator import RLAlphaGenerator\n",
    "\n",
    "    return {\n",
    "        \"load_market_data\": load_market_data,\n",
    "        \"AlphaTokenizer\": AlphaTokenizer,\n",
    "        \"AlphaCombinationModel\": AlphaCombinationModel,\n",
    "        \"AlphaGenerationEnv\": AlphaGenerationEnv,\n",
    "        \"RLAlphaGenerator\": RLAlphaGenerator,\n",
    "    }\n",
    "\n",
    "components = reload_components()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9518107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \n",
      "[1, 6] close\n",
      "[1, 6, 11] close 10\n",
      "[1, 6, 11, 30] close 10 ts_var\n",
      "[1, 6, 11, 30, 3] close 10 ts_var open\n",
      "[1, 6, 11, 30, 3, 7] close 10 ts_var open volume\n",
      "[1, 6, 11, 30, 3, 7, 36] close 10 ts_var open volume /\n",
      "[1, 6, 11, 30, 3, 7, 36, 3] close 10 ts_var open volume / open\n",
      "[1, 6, 11, 30, 3, 7, 36, 3, 3] close 10 ts_var open volume / open open\n",
      "[1, 6, 11, 30, 3, 7, 36, 3, 3, 36] close 10 ts_var open volume / open open /\n",
      "[1, 6, 11, 30, 3, 7, 36, 3, 3, 36, 6] close 10 ts_var open volume / open open / close\n",
      "[1, 6, 11, 30, 3, 7, 36, 3, 3, 36, 6, 34] close 10 ts_var open volume / open open / close -\n",
      "[1, 6, 11, 30, 3, 7, 36, 3, 3, 36, 6, 34, 11] close 10 ts_var open volume / open open / close - 10\n",
      "[1, 6, 11, 30, 3, 7, 36, 3, 3, 36, 6, 34, 11, 21] close 10 ts_var open volume / open open / close - 10 ts_mad\n",
      "[1, 6, 11, 30, 3, 7, 36, 3, 3, 36, 6, 34, 11, 21, 14] close 10 ts_var open volume / open open / close - 10 ts_mad 0.5\n",
      "[1, 6, 11, 30, 3, 7, 36, 3, 3, 36, 6, 34, 11, 21, 14, 20] close 10 ts_var open volume / open open / close - 10 ts_mad 0.5 ts_ema\n",
      "[1, 6, 11, 30, 3, 7, 36, 3, 3, 36, 6, 34, 11, 21, 14, 20, 36] close 10 ts_var open volume / open open / close - 10 ts_mad 0.5 ts_ema /\n",
      "[1, 6, 11, 30, 3, 7, 36, 3, 3, 36, 6, 34, 11, 21, 14, 20, 36, 36] close 10 ts_var open volume / open open / close - 10 ts_mad 0.5 ts_ema / /\n",
      "close 10 ts_var open volume / open open / close - 10 ts_mad 0.5 ts_ema / /\n",
      "[1] \n",
      "[1, 9] 3\n",
      "[1, 9, 9] 3 3\n",
      "3 3\n",
      "[1, 9, 9, 2] (tensor([[[ 0.1807,  0.2380,  0.2135, -0.1161, -0.1306,  0.0683, -0.0440,\n",
      "          -0.0302, -0.1050, -0.0129,  0.1073,  0.0303,  0.2018, -0.1416,\n",
      "          -0.0701, -0.1210,  0.0615,  0.0061, -0.0382,  0.0038, -0.0068,\n",
      "           0.0038,  0.2001, -0.2077, -0.0491,  0.0689, -0.0748, -0.0292,\n",
      "           0.3096,  0.0329,  0.1914, -0.2635, -0.0909, -0.1385,  0.1928,\n",
      "           0.0684, -0.0355, -0.0184, -0.1509,  0.0021,  0.2699,  0.2742,\n",
      "           0.2445, -0.1735,  0.0164,  0.0206,  0.0016,  0.1686,  0.0496,\n",
      "          -0.1043,  0.0666,  0.0932, -0.0564, -0.1115, -0.2513,  0.0119,\n",
      "          -0.1865,  0.0709,  0.3595, -0.1006, -0.2722, -0.1361, -0.0566,\n",
      "          -0.0788]]], grad_fn=<StackBackward0>), tensor([[[ 0.6046,  0.4766,  0.6383, -0.1668, -0.2760,  0.1443, -0.1888,\n",
      "          -0.0466, -0.1847, -0.0242,  0.1342,  0.0824,  0.3898, -0.3532,\n",
      "          -0.0973, -0.2278,  0.1153,  0.0120, -0.0719,  0.0077, -0.0125,\n",
      "           0.0085,  0.6118, -0.4557, -0.0862,  0.1260, -0.1934, -0.1057,\n",
      "           0.8527,  0.1027,  0.3636, -0.5122, -0.1598, -0.1666,  0.5781,\n",
      "           0.1621, -0.1051, -0.0776, -0.2986,  0.0047,  0.4917,  0.4099,\n",
      "           0.3472, -0.4008,  0.0640,  0.0508,  0.0032,  0.4414,  0.1427,\n",
      "          -0.2022,  0.1644,  0.1627, -0.1346, -0.2326, -0.5423,  0.0183,\n",
      "          -0.3622,  0.0923,  0.7021, -0.1336, -0.5704, -0.2487, -0.1567,\n",
      "          -0.1387]]], grad_fn=<StackBackward0>))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     22\u001b[39m agent = RLAlphaGenerator(env, cfg)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# TODO: 修正 bug\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 不训练，只采样\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m s, a, logp, ret, adv = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_collect_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample states shape:\u001b[39m\u001b[33m\"\u001b[39m, s.shape)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample actions shape:\u001b[39m\u001b[33m\"\u001b[39m, a.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kyle\\Desktop\\workspace\\factor_machine\\generator.py:246\u001b[39m, in \u001b[36mRLAlphaGenerator._collect_trajectories\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    244\u001b[39m logp = torch.tensor(\u001b[32m0.0\u001b[39m, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    245\u001b[39m \u001b[38;5;28mprint\u001b[39m(obs, h_v)\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m value, h_v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_v\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m reward = -\u001b[32m1.0\u001b[39m\n\u001b[32m    248\u001b[39m done = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kyle\\Desktop\\workspace\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kyle\\Desktop\\workspace\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kyle\\Desktop\\workspace\\factor_machine\\generator.py:89\u001b[39m, in \u001b[36mValueNetwork.forward\u001b[39m\u001b[34m(self, x, hidden)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m, x: torch.Tensor, hidden: Tuple[torch.Tensor, torch.Tensor]\n\u001b[32m     79\u001b[39m ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\u001b[32m     80\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m    参数:\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m        x: 输入 token 序列，形状为 (batch_size, seq_len)。\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m \u001b[33;03m        hidden: 更新后的隐藏状态。\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     out, hidden = \u001b[38;5;28mself\u001b[39m.lstm(emb, hidden)\n\u001b[32m     91\u001b[39m     value = \u001b[38;5;28mself\u001b[39m.fc_value(out[:, -\u001b[32m1\u001b[39m, :]).squeeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kyle\\Desktop\\workspace\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kyle\\Desktop\\workspace\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kyle\\Desktop\\workspace\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kyle\\Desktop\\workspace\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: embedding(): argument 'indices' (position 2) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "from generator import RLAlphaGenerator\n",
    "from envs import AlphaGenerationEnv\n",
    "from combination import AlphaCombinationModel\n",
    "from tokenizer import AlphaTokenizer\n",
    "from data import load_market_data\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "df = load_market_data()\n",
    "combo = AlphaCombinationModel()\n",
    "combo.inject_data(df, \"target\")\n",
    "tokenizer = AlphaTokenizer()\n",
    "env = AlphaGenerationEnv(combo, tokenizer, max_len=20)\n",
    "\n",
    "cfg = dict(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_dim=64,\n",
    "    batch_size=32,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "agent = RLAlphaGenerator(env, cfg)\n",
    "\n",
    "# TODO: 修正 bug\n",
    "# 不训练，只采样\n",
    "s, a, logp, ret, adv = agent._collect_trajectories()\n",
    "print(\"Sample states shape:\", s.shape)\n",
    "print(\"Sample actions shape:\", a.shape)\n",
    "print(\"Sample rewards (returns):\", ret[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7326d7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
