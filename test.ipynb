{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f16224",
   "metadata": {},
   "source": [
    "### è¿™ä¸ª notebook ç”¨æ¥è¯»å–æ–‡ä»¶ï¼Œå¹¶æµ‹è¯•ä¸€äº›æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb46bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ æ‰€æœ‰ Python æ–‡ä»¶å†…å®¹å·²å†™å…¥ src.txtï¼ˆå…± 11 ä¸ªæ–‡ä»¶ï¼‰\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_all_py_files(root_dir: str) -> list:\n",
    "    \"\"\"\n",
    "    è·å–æŒ‡å®šç›®å½•åŠå…¶å­ç›®å½•ä¸‹çš„æ‰€æœ‰ .py æ–‡ä»¶è·¯å¾„ã€‚\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): èµ·å§‹ç›®å½•è·¯å¾„ã€‚\n",
    "\n",
    "    Returns:\n",
    "        list: åŒ…å«æ‰€æœ‰ .py æ–‡ä»¶å®Œæ•´è·¯å¾„çš„åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    py_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".py\"):\n",
    "                py_files.append(os.path.join(dirpath, file))\n",
    "    return py_files\n",
    "\n",
    "def write_all_py_contents_to_output(py_files: list, output_path: str = \"src.txt\") -> None:\n",
    "    \"\"\"\n",
    "    å°†æ‰€æœ‰ .py æ–‡ä»¶çš„å†…å®¹å†™å…¥ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­ï¼Œå¹¶æ‰“å°æ–‡ä»¶åä½œä¸ºåˆ†éš”ã€‚\n",
    "\n",
    "    Args:\n",
    "        py_files (list): .py æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚\n",
    "        output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        for path in py_files:\n",
    "            out_file.write(f\"{'=' * 80}\\n\")\n",
    "            out_file.write(f\"File: {path}\\n\")\n",
    "            out_file.write(f\"{'-' * 80}\\n\")\n",
    "            try:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    out_file.write(f.read())\n",
    "            except Exception as e:\n",
    "                out_file.write(f\"âš ï¸ Error reading {path}: {e}\\n\")\n",
    "            out_file.write(\"\\n\\n\")\n",
    "\n",
    "if True:\n",
    "    root_directory = \".\"  # å½“å‰ç›®å½•\n",
    "    all_py_files = get_all_py_files(root_directory)\n",
    "    write_all_py_contents_to_output(all_py_files)\n",
    "    print(f\"ğŸ“„ æ‰€æœ‰ Python æ–‡ä»¶å†…å®¹å·²å†™å…¥ src.txtï¼ˆå…± {len(all_py_files)} ä¸ªæ–‡ä»¶ï¼‰\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3581e",
   "metadata": {},
   "source": [
    "### æ£€æŸ¥ tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194f4d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 10, 28, 2]\n",
      "open 5 ts_std\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import AlphaTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tok = AlphaTokenizer()\n",
    "\n",
    "expr = \"open 5 ts_std\"          # ç›¸å½“äº ts_mean(close, 5)\n",
    "\n",
    "ids = tok.encode(expr)\n",
    "print(ids)        # âœ [BOS]  å¯¹åº” id,  ... , [SEP]\n",
    "\n",
    "print(tok.decode(ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ddfd39",
   "metadata": {},
   "source": [
    "### 1.æµ‹è¯• AlphaCombinationModel._compute_alpha_from_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70fb8b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha shape: (64861,)\n",
      "alpha sample: [2971.82 2971.85 2971.83 2971.81 2971.81]\n"
     ]
    }
   ],
   "source": [
    "from combination import AlphaCombinationModel\n",
    "from data import load_market_data\n",
    "\n",
    "df = load_market_data()\n",
    "model = AlphaCombinationModel()\n",
    "model.inject_data(df, target_col='target')\n",
    "\n",
    "# è¡¨è¾¾å¼ï¼šclose çš„ 100 ç§’å‡å€¼\n",
    "expr = \"close 100 ts_mean\"\n",
    "alpha = model._compute_alpha_from_expr(expr)\n",
    "\n",
    "print(\"alpha shape:\", alpha.shape)\n",
    "print(\"alpha sample:\", alpha[~np.isnan(alpha)][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bc3e9",
   "metadata": {},
   "source": [
    "###  2. æµ‹è¯• AlphaCombinationModel.add_alpha_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc603c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯¥å› å­çš„ IC ä¸ºï¼š 0.028037411051210277\n",
      "å½“å‰æ± ä¸­å› å­æ•°ï¼š 1\n"
     ]
    }
   ],
   "source": [
    "ic = model.add_alpha_expr(\"high low - 100 ts_max\")\n",
    "print(\"è¯¥å› å­çš„ IC ä¸ºï¼š\", ic)\n",
    "print(\"å½“å‰æ± ä¸­å› å­æ•°ï¼š\", len(model.alphas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba8fd7",
   "metadata": {},
   "source": [
    "### 3. æµ‹è¯• AlphaTokenizer.encode / decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda7d163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1, 6, 10, 23, 2]\n",
      "Decoded expr: close 5 ts_mean\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import AlphaTokenizer\n",
    "\n",
    "tokenizer = AlphaTokenizer()\n",
    "\n",
    "expr = \"close 5 ts_mean\"\n",
    "ids = tokenizer.encode(expr)\n",
    "decoded = tokenizer.decode(ids)\n",
    "\n",
    "print(\"Token IDs:\", ids)\n",
    "print(\"Decoded expr:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1899091",
   "metadata": {},
   "source": [
    "### 4. æµ‹è¯• AlphaGenerationEnv.reset / step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5018150f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå§‹çŠ¶æ€ token IDs: [1]\n",
      "æ–°çŠ¶æ€: [1, 3]\n",
      "Reward: 0.0 Done: False\n"
     ]
    }
   ],
   "source": [
    "from envs import AlphaGenerationEnv\n",
    "from combination import AlphaCombinationModel\n",
    "from tokenizer import AlphaTokenizer\n",
    "from data import load_market_data\n",
    "\n",
    "df = load_market_data()\n",
    "combo = AlphaCombinationModel()\n",
    "combo.inject_data(df, target_col='target')\n",
    "tokenizer = AlphaTokenizer()\n",
    "env = AlphaGenerationEnv(combo, tokenizer)\n",
    "\n",
    "obs = env.reset()\n",
    "print(\"åˆå§‹çŠ¶æ€ token IDs:\", obs)\n",
    "\n",
    "# å°è¯•ä¸€æ­¥åŠ¨ä½œï¼ˆåˆæ³•åŠ¨ä½œä¸­ä»»é€‰ä¸€ä¸ªï¼‰\n",
    "valid = env.valid_actions()\n",
    "action = valid[0]\n",
    "obs2, reward, done, info = env.step(action)\n",
    "print(\"æ–°çŠ¶æ€:\", obs2)\n",
    "print(\"Reward:\", reward, \"Done:\", done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1291c",
   "metadata": {},
   "source": [
    "### 5. æµ‹è¯• PolicyNetwork / ValueNetwork è¾“å‡ºç»´åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90c5b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy logits shape: torch.Size([1, 50])\n",
      "Value estimate shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from generator import PolicyNetwork, ValueNetwork\n",
    "\n",
    "vocab_size = 50\n",
    "seq_len = 6\n",
    "hidden_dim = 64\n",
    "device = \"cpu\"\n",
    "\n",
    "x = torch.randint(0, vocab_size, (1, seq_len))  # batch_size=1\n",
    "policy = PolicyNetwork(vocab_size, hidden_dim).to(device)\n",
    "value = ValueNetwork(vocab_size, hidden_dim).to(device)\n",
    "\n",
    "h0_p = policy.init_hidden(1, device)\n",
    "logits, _ = policy(x, h0_p)\n",
    "print(\"Policy logits shape:\", logits.shape)  # åº”ä¸º (1, vocab_size)\n",
    "\n",
    "h0_v = value.init_hidden(1, device)\n",
    "v, _ = value(x, h0_v)\n",
    "print(\"Value estimate shape:\", v.shape)      # åº”ä¸º (1,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b7e9d",
   "metadata": {},
   "source": [
    "### 6. æµ‹è¯• RLAlphaGenerator._collect_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b861b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def reload_components():\n",
    "    \"\"\"\n",
    "    é‡æ–°åŠ è½½ä»¥ä¸‹æ¨¡å—ï¼Œä»¥ä¾¿åœ¨å¼€å‘è¿‡ç¨‹ä¸­å³æ—¶ç”Ÿæ•ˆï¼š\n",
    "      - data.load_market_data\n",
    "      - tokenizer.AlphaTokenizer\n",
    "      - combination.AlphaCombinationModel\n",
    "      - envs.AlphaGenerationEnv\n",
    "      - generator.RLAlphaGenerator\n",
    "    \"\"\"\n",
    "    import data, tokenizer, combination, envs, generator\n",
    "\n",
    "    importlib.reload(data)\n",
    "    importlib.reload(tokenizer)\n",
    "    importlib.reload(combination)\n",
    "    importlib.reload(envs)\n",
    "    importlib.reload(generator)\n",
    "\n",
    "    # é‡æ–°ç»‘å®šåˆ°æœ¬åœ°åç§°ï¼ˆå¯é€‰ï¼‰\n",
    "    from data import load_market_data\n",
    "    from tokenizer import AlphaTokenizer\n",
    "    from combination import AlphaCombinationModel\n",
    "    from envs import AlphaGenerationEnv\n",
    "    from generator import RLAlphaGenerator\n",
    "\n",
    "    return {\n",
    "        \"load_market_data\": load_market_data,\n",
    "        \"AlphaTokenizer\": AlphaTokenizer,\n",
    "        \"AlphaCombinationModel\": AlphaCombinationModel,\n",
    "        \"AlphaGenerationEnv\": AlphaGenerationEnv,\n",
    "        \"RLAlphaGenerator\": RLAlphaGenerator,\n",
    "    }\n",
    "\n",
    "components = reload_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9518107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high low * 20 high\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "RPN è¡¨è¾¾å¼æœ€ç»ˆæ ˆæ·±åº¦åº”ä¸º 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     20\u001b[39m agent = RLAlphaGenerator(env, cfg)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: ä¿®æ­£ bug\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ä¸è®­ç»ƒï¼Œåªé‡‡æ ·\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m s, a, logp, ret, adv = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_collect_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample states shape:\u001b[39m\u001b[33m\"\u001b[39m, s.shape)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample actions shape:\u001b[39m\u001b[33m\"\u001b[39m, a.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/workspace/25summer/rl-factor-machine/generator.py:241\u001b[39m, in \u001b[36mRLAlphaGenerator._collect_trajectories\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid) == \u001b[32m0\u001b[39m:\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# æ— åˆæ³•åŠ¨ä½œ â†’ å¼ºåˆ¶æ”¶å°¾å¹¶æƒ©ç½š\u001b[39;00m\n\u001b[32m    240\u001b[39m     sep_id = \u001b[38;5;28mself\u001b[39m.env.tokenizer.sep_token_id\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     obs, _, _, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msep_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m     action = torch.tensor(sep_id, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    243\u001b[39m     logp = torch.tensor(\u001b[32m0.0\u001b[39m, device=\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/workspace/25summer/rl-factor-machine/envs.py:77\u001b[39m, in \u001b[36mAlphaGenerationEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action == \u001b[38;5;28mself\u001b[39m.tokenizer.sep_token_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.sequence) >= \u001b[38;5;28mself\u001b[39m.max_len:\n\u001b[32m     76\u001b[39m     expr = \u001b[38;5;28mself\u001b[39m.tokenizer.decode(\u001b[38;5;28mself\u001b[39m.sequence, remove_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     reward = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombo_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     79\u001b[39m obs = \u001b[38;5;28mself\u001b[39m._get_obs()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/workspace/25summer/rl-factor-machine/combination.py:156\u001b[39m, in \u001b[36mAlphaCombinationModel.evaluate_alpha\u001b[39m\u001b[34m(self, expr)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03mç›´æ¥æ ¹æ® RPN è¡¨è¾¾å¼è®¡ç®—å¹¶è¿”å›å•å› å­ ICï¼ˆå¸¦ç¼“å­˜ï¼‰ã€‚\u001b[39;00m\n\u001b[32m    145\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m \u001b[33;03m    ValueError: å½“è¡¨è¾¾å¼è§£ææˆ–è¿ç®—å¤±è´¥æ—¶ã€‚\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mprint\u001b[39m(expr)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m new_alpha = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_alpha_from_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m norm = \u001b[38;5;28mself\u001b[39m._maybe_normalize(new_alpha)\n\u001b[32m    158\u001b[39m key = (\u001b[33m'\u001b[39m\u001b[33mexpr_ic\u001b[39m\u001b[33m'\u001b[39m, expr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/workspace/25summer/rl-factor-machine/combination.py:252\u001b[39m, in \u001b[36mAlphaCombinationModel._compute_alpha_from_expr\u001b[39m\u001b[34m(self, expr)\u001b[39m\n\u001b[32m    249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mæœªçŸ¥ tokenï¼š\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stack) != \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRPN è¡¨è¾¾å¼æœ€ç»ˆæ ˆæ·±åº¦åº”ä¸º 1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# è¿”å› numpy æ•°ç»„ï¼Œåç»­ä¼šåš winsorize + z-score\u001b[39;00m\n\u001b[32m    254\u001b[39m series = stack[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: RPN è¡¨è¾¾å¼æœ€ç»ˆæ ˆæ·±åº¦åº”ä¸º 1"
     ]
    }
   ],
   "source": [
    "from generator import RLAlphaGenerator\n",
    "from envs import AlphaGenerationEnv\n",
    "from combination import AlphaCombinationModel\n",
    "from tokenizer import AlphaTokenizer\n",
    "from data import load_market_data\n",
    "\n",
    "df = load_market_data()\n",
    "combo = AlphaCombinationModel()\n",
    "combo.inject_data(df, \"target\")\n",
    "tokenizer = AlphaTokenizer()\n",
    "env = AlphaGenerationEnv(combo, tokenizer, max_len=20)\n",
    "\n",
    "cfg = dict(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_dim=64,\n",
    "    batch_size=32,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "agent = RLAlphaGenerator(env, cfg)\n",
    "\n",
    "# TODO: ä¿®æ­£ bug\n",
    "# ä¸è®­ç»ƒï¼Œåªé‡‡æ ·\n",
    "s, a, logp, ret, adv = agent._collect_trajectories()\n",
    "print(\"Sample states shape:\", s.shape)\n",
    "print(\"Sample actions shape:\", a.shape)\n",
    "print(\"Sample rewards (returns):\", ret[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7326d7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
