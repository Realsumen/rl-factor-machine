# %% [markdown]
# ### è¿™ä¸ª notebook ç”¨æ¥è¯»å–æ–‡ä»¶ï¼Œå¹¶æµ‹è¯•ä¸€äº›æ–¹æ³•

# %%
import os
import numpy as np


def get_all_py_files(root_dir: str) -> list:
    """
    è·å–æŒ‡å®šç›®å½•åŠå…¶å­ç›®å½•ä¸‹çš„æ‰€æœ‰ .py æ–‡ä»¶è·¯å¾„ã€‚

    Args:
        root_dir (str): èµ·å§‹ç›®å½•è·¯å¾„ã€‚

    Returns:
        list: åŒ…å«æ‰€æœ‰ .py æ–‡ä»¶å®Œæ•´è·¯å¾„çš„åˆ—è¡¨ã€‚
    """
    py_files = []
    for dirpath, _, filenames in os.walk(root_dir):
        for file in filenames:
            if file.endswith(".py"):
                py_files.append(os.path.join(dirpath, file))
    return py_files


def write_all_py_contents_to_output(
    py_files: list, output_path: str = "src.txt"
) -> None:
    """
    å°†æ‰€æœ‰ .py æ–‡ä»¶çš„å†…å®¹å†™å…¥ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­ï¼Œå¹¶æ‰“å°æ–‡ä»¶åä½œä¸ºåˆ†éš”ã€‚

    Args:
        py_files (list): .py æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
        output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚
    """
    with open(output_path, "w", encoding="utf-8") as out_file:
        for path in py_files:
            out_file.write(f"{'=' * 80}\n")
            out_file.write(f"File: {path}\n")
            out_file.write(f"{'-' * 80}\n")
            try:
                with open(path, "r", encoding="utf-8") as f:
                    out_file.write(f.read())
            except Exception as e:
                out_file.write(f"âš ï¸ Error reading {path}: {e}\n")
            out_file.write("\n\n")


if True:
    root_directory = "."  # å½“å‰ç›®å½•
    all_py_files = get_all_py_files(root_directory)
    write_all_py_contents_to_output(all_py_files)
    print(f"ğŸ“„ æ‰€æœ‰ Python æ–‡ä»¶å†…å®¹å·²å†™å…¥ src.txtï¼ˆå…± {len(all_py_files)} ä¸ªæ–‡ä»¶ï¼‰")


# %%
import importlib
from utility import set_random_seed


def reload_components():
    """
    é‡æ–°åŠ è½½ä»¥ä¸‹æ¨¡å—ï¼Œä»¥ä¾¿åœ¨å¼€å‘è¿‡ç¨‹ä¸­å³æ—¶ç”Ÿæ•ˆï¼š
      - data.load_market_data
      - tokenizer.AlphaTokenizer
      - combination.AlphaCombinationModel
      - envs.AlphaGenerationEnv
      - generator.RLAlphaGenerator
    """
    import data, tokenizer, combination, alpha_generation_env, generator

    importlib.reload(data)
    importlib.reload(tokenizer)
    importlib.reload(combination)
    importlib.reload(alpha_generation_env)
    importlib.reload(generator)

    # é‡æ–°ç»‘å®šåˆ°æœ¬åœ°åç§°ï¼ˆå¯é€‰ï¼‰
    from data import load_market_data
    from tokenizer import AlphaTokenizer
    from combination import AlphaCombinationModel
    from alpha_generation_env import AlphaGenerationEnv
    from generator import RLAlphaGenerator

    return {
        "load_market_data": load_market_data,
        "AlphaTokenizer": AlphaTokenizer,
        "AlphaCombinationModel": AlphaCombinationModel,
        "AlphaGenerationEnv": AlphaGenerationEnv,
        "RLAlphaGenerator": RLAlphaGenerator,
    }


components = reload_components()


# %% [markdown]
# ### 1.æµ‹è¯• AlphaCombinationModel._compute_alpha_from_expr

# %%
from combination import AlphaCombinationModel
from data import load_market_data

df = load_market_data()
model = AlphaCombinationModel()
model.inject_data(df, target_col="target")

# è¡¨è¾¾å¼ï¼šclose çš„ 100 ç§’å‡å€¼
expr = "close 100 ts_mean"
alpha = model._compute_alpha_from_expr(expr)

print("alpha shape:", alpha.shape)
print("alpha sample:", alpha[~np.isnan(alpha)][:5])


# %% [markdown]
# ###  2. æµ‹è¯• AlphaCombinationModel.add_alpha_expr

# %%
ic = model.add_alpha_expr("high low sub 100 ts_max")
print("è¯¥å› å­çš„ IC ä¸ºï¼š", ic)
print("å½“å‰æ± ä¸­å› å­æ•°ï¼š", len(model.alphas))


# %% [markdown]
# ### 3. æµ‹è¯• AlphaTokenizer.encode / decode

# %%
from tokenizer import AlphaTokenizer

tokenizer = AlphaTokenizer()

expr = "close 5 ts_mean"
ids = tokenizer.encode(expr)
decoded = tokenizer.decode(ids)

print("Token IDs:", ids)
print("Decoded expr:", decoded)


# %% [markdown]
# ### 4. æµ‹è¯• AlphaGenerationEnv.reset / step

# %%
from alpha_generation_env import AlphaGenerationEnv
from combination import AlphaCombinationModel
from tokenizer import AlphaTokenizer
from data import load_market_data

df = load_market_data()
combo = AlphaCombinationModel()
combo.inject_data(df, target_col="target")
tokenizer = AlphaTokenizer()
env = AlphaGenerationEnv(combo, tokenizer)

obs = env.reset()
print("åˆå§‹çŠ¶æ€ token IDs:", obs)

valid = env.valid_actions()
action = valid[1]
obs2, reward, done, info = env.step(action)
print("æ–°çŠ¶æ€:", obs2)
print("Reward:", reward, "Done:", done)


# %% [markdown]
# ### 5. æµ‹è¯• PolicyNetwork / ValueNetwork è¾“å‡ºç»´åº¦

# %%
import torch
from generator import PolicyNetwork, ValueNetwork

vocab_size = 50
seq_len = 6
hidden_dim = 64
device = "cpu"

x = torch.randint(0, vocab_size, (1, seq_len))  # batch_size=1
policy = PolicyNetwork(vocab_size, hidden_dim).to(device)
value = ValueNetwork(vocab_size, hidden_dim).to(device)

h0_p = policy.init_hidden(1, device)
logits, _ = policy(x, h0_p)
print("Policy logits shape:", logits.shape)  # åº”ä¸º (1, vocab_size)

h0_v = value.init_hidden(1, device)
v, _ = value(x, h0_v)
print("Value estimate shape:", v.shape)  # åº”ä¸º (1,)


# %% [markdown]
# ### 6. æµ‹è¯• RLAlphaGenerator._collect_trajectories

# %%
from generator import RLAlphaGenerator
from alpha_generation_env import AlphaGenerationEnv
from combination import AlphaCombinationModel
from tokenizer import AlphaTokenizer
from data import load_market_data
from utility import set_random_seed

# reload_components()

set_random_seed(10)

df = load_market_data()
combo = AlphaCombinationModel()
combo.inject_data(df, "target")
tokenizer = AlphaTokenizer()
env = AlphaGenerationEnv(combo, tokenizer, max_len=20)

cfg = dict(
    vocab_size=tokenizer.vocab_size,
    hidden_dim=64,
    batch_size=1280,
    device="cpu",
)

agent = RLAlphaGenerator(env, cfg)

s, a, logp, ret, adv = agent._collect_trajectories()
print("Sample states shape:", s.shape)
print("Sample actions shape:", a.shape)
print("Sample rewards (returns):", ret.shape, ret)


# %%
