================================================================================
File: ./envs.py
--------------------------------------------------------------------------------
# env.py
from combination import AlphaCombinationModel
from tokenizer import AlphaTokenizer
from typing import List


class AlphaGenerationEnv:
    """
    自定义强化学习环境：生成逆波兰表达式（RPN）的序列决策过程。

    - **状态**：当前已生成的 token ID 列表
    - **动作**：下一个 token 的 ID
    - **奖励**：组合模型评估的单因子 IC
    - **终止条件**：生成 `[SEP]` 或达到 `max_len`
    """

    def __init__(
        self, combo_model: AlphaCombinationModel, tokenizer: AlphaTokenizer, max_len=20
    ):
        """
        初始化环境。

        参数
        ----------
        combo_model : AlphaCombinationModel
            负责因子池管理与 IC 计算的组合模型。
        tokenizer : AlphaTokenizer
            RPN ↔ token 序列转换器。
        max_len : int, 可选
            生成序列的最⼤长度（含 `[BOS]` 与 `[SEP]`），默认 20。
        """
        self.combo_model = combo_model
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.reset()

    def reset(self):
        """
        重新开始一条新序列。

        返回
        ----------
        List[int]
            仅包含 `[BOS]` 的初始序列。
        """
        self.sequence = [self.tokenizer.bos_token_id]
        self.done = False
        return self._get_obs()

    def step(self, action: int):
        """
        执行一步生成并返回环境转移结果。

        参数
        ----------
        action : int
            选定的 token ID。

        返回
        ----------
        obs : List[int]
            新的 token 序列。
        reward : float
            若已结束则为该表达式的单因子 IC，否则为 0。
        done : bool
            是否到达终止状态。
        info : dict
            预留调试信息，当前为空字典。
        """
        self.sequence.append(action)
        token = self.tokenizer.decode([action])
        reward = 0.0
        if action == self.tokenizer.sep_token_id or len(self.sequence) >= self.max_len:
            expr = self.tokenizer.decode(self.sequence[1:-1])
            reward = self.combo_model.evaluate_alpha(expr)
            self.done = True
        obs = self._get_obs()
        return obs, reward, self.done, {}

    def _get_obs(self):
        """
        获取当前观测值（token 序列）。

        返回
        ----------
        List[int]
            当前生成序列（含 `[BOS]`，可能含 `[SEP]`）。
        """
        return self.sequence

    def valid_actions(self) -> List[int]:
        """
        计算当前状态下的合法动作集合，用于 Invalid-Action-Mask。

        规则（基于逆波兰表达式 RPN）
        ------------------------------------------------------------
        1. 维护一个“栈深”计数：
           • 操作数（基础字段或常量）   → 栈深 +1
           • 算子（arity = n）         → 栈深 −(n-1)
        2. 任何前缀片段都必须满足栈深 ≥ 0（否则语法非法）。
        3. 仅当栈深 == 1 时才允许收尾 `[SEP]`。
        4. 生成过程中禁止再次选择 `[BOS]`、`[PAD]`。
        5. 若已接近最大长度，只允许 `[SEP]`。
        """
        # ---------------- 终止态快速返回 ----------------
        if self.done:
            return []  # Episode 已结束

        # ---------- 1. 当前栈深 -------------------------------------------------
        #    跳过首个 [BOS]；若最后已是 [SEP]，说明逻辑上应已 done，但稳妥起见再校验
        sep_str = self.tokenizer.id_to_token[self.tokenizer.sep_token_id]
        tokens = [self.tokenizer.id_to_token[t] for t in self.sequence[1:]]
        if tokens and tokens[-1] == sep_str:
            return []

        #   构建 「算子 → arity」 映射（含四则运算）
        import inspect, operators

        op_arity = {"+": 2, "-": 2, "*": 2, "/": 2}
        for name, fn in inspect.getmembers(operators, inspect.isfunction):
            op_arity[name] = len(inspect.signature(fn).parameters)

        def _delta(tok: str) -> int:
            """token 对栈深的增量"""
            if tok in op_arity:  # 算子
                return 1 - op_arity[tok]
            if tok in self.tokenizer.special_tokens:  # [BOS]/[SEP]/[PAD]
                return 0
            return 1  # 操作数

        stack_depth = 0
        for tk in tokens:
            stack_depth += _delta(tk)

         # ---------- 2. 若长度已至上限-1，则只能收尾 ------------------------------
        if len(self.sequence) >= self.max_len - 1:
            return [self.tokenizer.sep_token_id]

        # ---------- 3. 枚举所有 token，筛选合法动作 -----------------------------
        valid: List[int] = []
        remaining_steps = self.max_len - len(self.sequence) - 1  # 预留结尾 [SEP]

        for tid in range(self.tokenizer.vocab_size):
            tok = self.tokenizer.id_to_token[tid]

            # 跳过无意义或禁止的特殊标记
            if tok in ("[BOS]", "[PAD]"):
                continue

            # ---- A. 结束符 `[SEP]` ----
            if tok == "[SEP]":
                if stack_depth == 1:               # 仅当栈深恰好为 1 可收尾
                    valid.append(tid)
                continue

            # ---- B. 普通 token ----
            nd = stack_depth + _delta(tok)
            # 合法性 1：中途栈深不得为负
            if nd < 0:
                continue
            # 合法性 2：后续仍需有可能在剩余步数内归结到栈深 1
            #   最悲观情形：后面全用二元算子，每步栈深 -1，再加结尾 [SEP]
            #   所需最少步 = (nd - 1)   （降到 1） + 1（收尾）
            if nd - 1 + 1 > remaining_steps:
                continue

            valid.append(tid)

        return valid


================================================================================
File: ./utility.py
--------------------------------------------------------------------------------
# utility.py

import numpy as np
import pandas as pd

def zscore_normalize(alpha: np.ndarray) -> pd.Series:
    """
    Z-score 标准化处理：消除因子的尺度影响
    """
    a = np.asarray(alpha, dtype=np.float64)
    # 用 nanmean/nanstd 自动跳过 NaN
    mean = np.nanmean(a)
    std = np.nanstd(a)
    # 如果全是 NaN 或者方差为 0
    if std == 0 or np.isnan(std):
        return pd.Series(np.zeros_like(a))
    return pd.Series((a - mean) / std)

def winsorize(alpha: np.ndarray, lower_quantile: float = 0.01, upper_quantile: float = 0.99) -> pd.Series:
    """
    异常值截断处理：防止 outlier 扰动因子稳定性
    """
    a = np.asarray(alpha, dtype=float)
    lower = np.nanpercentile(a, lower_quantile * 100)
    upper = np.nanpercentile(a, upper_quantile * 100)
    clipped = np.clip(a, lower, upper)
    return pd.Series(clipped)

def information_coefficient(factor: np.ndarray, target: np.ndarray) -> float:
    """
    信息系数（IC）：衡量因子预测未来收益的能力，Pearson 相关系数
    """
    if len(factor) != len(target):
        raise ValueError("Factor and target length mismatch")
    
    # 1. 同时有效（非 NaN/Inf）的掩码
    mask = np.isfinite(factor) & np.isfinite(target)
    if mask.sum() < 2:               # 样本太少无法算相关
        return 0.0
    
    f, t = factor[mask], target[mask]

    # 2. σ=0 说明缺乏波动，相关性定义无意义，直接返回 0
    if np.std(f) == 0 or np.std(t) == 0:
        return 0.0

    return float(np.corrcoef(f, t)[0, 1])

================================================================================
File: ./__init__.py
--------------------------------------------------------------------------------


================================================================================
File: ./combination.py
--------------------------------------------------------------------------------
# combination.py
import numpy as np
from utility import zscore_normalize, winsorize, information_coefficient
from scipy.optimize import minimize
from typing import List, Dict, Union, Tuple, Callable
import operators
import pandas as pd
import inspect

class AlphaCombinationModel:
    """
    因子线性组合管理器，实现论文中算法1的核心思想。

    功能:
      - 维护一个最多包含 `max_pool_size` 条归一化因子序列的因子池。
      - 每当有新因子生成时，执行截尾（Winsorize）和 Z-score 标准化，并计算该因子的单因子 IC（Information Coefficient）。
      - 将新因子加入因子池后，通过凸优化（SLSQP）求解最优线性权重，以最大化组合 IC。
      - 若因子池超过上限，则剔除对组合贡献度最小的因子。
      - 在多次计算中对标准化序列、IC 值、最优权重等中间结果进行缓存，以减少重复开销。

    Attributes:
        max_pool_size (int): 因子池最大容量。超过后将删除贡献最小的因子。
        alphas (List[np.ndarray]): 原始因子序列列表。
        norm_alphas (List[np.ndarray]): 归一化后的因子序列列表。
        ic_list (List[float]): 对应每条因子的单因子 IC 列表。
        weights (List[float]): 当前组合中各因子的线性权重。
        expr_list (List[str]): 保存每条因子对应的 RPN 表达式。
        _cache (Dict): 缓存用于存储中间计算结果。
        data (pd.DataFrame): 注入的行情数据。
        _target (np.ndarray): 注入的目标列（未来收益）数组。
    """
    def __init__(self, max_pool_size: int = 50):
        """
        初始化 AlphaCombinationModel。

        Args:
            max_pool_size (int): 因子池的最大容量，上限内优先保留贡献度高的因子。
        """
        self.max_pool_size = max_pool_size
        self.alphas = []         # 原始因子序列列表
        self.norm_alphas = []    # 归一化后的因子序列列表
        self.ic_list = []        # 对应的单因子 IC 值列表
        self.weights = []        # 当前组合的线性权重列表
        self._cache = {}         # 缓存字典，用于存储中间结果
        self.expr_list = []      # 新增：对应每条因子的 RPN 表达式字符串

    def inject_data(self, df: pd.DataFrame, target_col: str) -> None:
        """
        注入市场行情数据和目标序列，用于 IC 计算与权重优化。

        Args:
            df (pd.DataFrame): 行情特征表，包含基础字段和目标列。
            target_col (str): DataFrame 中代表未来收益的列名，用于 IC 计算。

        Raises:
            ValueError: 当 target_col 不在 df 列时抛出。
        """
        # TODO: 需要更细致的训练集 / 验证集 分割逻辑，添加新的数据集作为验证集 etc
        self.data: pd.DataFrame = df
        self._target = df[target_col].values.astype(np.float64)


    def update_with(self, new_alpha: np.ndarray, expr: str):
        """
        将新因子加入池中并更新组合：
          1. 对原始因子序列做 winsorize 和 z-score 标准化。
          2. 计算并缓存该因子的 IC。
          3. 添加至因子池后，重优化线性权重。
          4. 超出容量时剔除贡献最小的因子。

        Args:
            new_alpha (np.ndarray): 新因子的原始序列数据，长度与注入的行情一致。
            expr (str): 生成该因子的 RPN 表达式字符串，用于记录及缓存键。
        """
        # 归一化
        norm = winsorize(zscore_normalize(new_alpha))
        key = ('ic', tuple(norm))
        if key in self._cache:
            ic = self._cache[key]
        else:
            target = self._load_validation_target()
            ic = information_coefficient(norm, target)
            self._cache[key] = ic

        # 更新因子池
        self.alphas.append(new_alpha)
        self.norm_alphas.append(norm)
        self.ic_list.append(ic)
        self.expr_list.append(expr)          # 记录表达式

        # 若是首因子直接赋权 1；否则重优化权重
        if len(self.norm_alphas) == 1:
            self.weights = [1.0]
        else:
            self._reoptimize_weights()

        # 超限时剔除贡献最小因子
        if len(self.alphas) > self.max_pool_size:
            contrib = [abs(w * ic) for w, ic in zip(self.weights, self.ic_list)]
            idx = contrib.index(min(contrib))
            for lst in (self.alphas, self.norm_alphas, self.ic_list, self.weights):
                lst.pop(idx)

    def add_alpha_expr(self, expr: str) -> float:
        """
        根据 RPN 表达式计算新因子，并将其加入因子池。

        Args:
            expr (str): RPN 格式的表达式，例如 "close 5 ts_mean"。

        Returns:
            float: 该因子的单 IC 值，可作为强化学习的 reward。

        Raises:
            ValueError: 当表达式格式错误或运算失败时。
        """
        new_alpha = self._compute_alpha_from_expr(expr)
        ic = self.evaluate_alpha(expr)                  # 里面自带缓存
        self.update_with(new_alpha, expr)
        return ic

    def _reoptimize_weights(self):
        """
        使用凸优化（SLSQP）在当前因子池上求解最优线性权重，
        目标：最大化组合序列与目标的 Pearson 相关系数（IC），
        约束：权重绝对值之和等于 1（L1 归一化）。
        """
        A = np.vstack(self.norm_alphas).T
        target = self._load_validation_target()

        def objective(w):
            combo = A.dot(w)
            ic = np.corrcoef(combo, target)[0, 1]
            return -np.nan_to_num(ic)

        cons = ({'type': 'eq', 'fun': lambda w: np.sum(np.abs(w)) - 1})
        x0 = np.ones(A.shape[1]) / A.shape[1]
        res = minimize(objective, x0, constraints=cons, method='SLSQP')
        self.weights = res.x.tolist()
        self._cache['weights'] = res.x.copy()

    def evaluate_alpha(self, expr: str) -> float:
        """
        直接根据 RPN 表达式计算并返回单因子 IC（带缓存）。

        Args:
            expr (str): RPN 表达式字符串。

        Returns:
            float: 该表达式生成因子的 Pearson IC。

        Raises:
            ValueError: 当表达式解析或运算失败时。
        """
        new_alpha = self._compute_alpha_from_expr(expr)
        norm = self._maybe_normalize(new_alpha)
        key = ('expr_ic', expr)
        if key not in self._cache:
            target = self._load_validation_target()
            self._cache[key] = information_coefficient(norm, target)
        return self._cache[key]

    def score(self) -> float:
        """
        计算当前因子组合在验证集上的加权 IC。

        Returns:
            float: 组合因子的 Pearson IC 值。
        """
        A = np.vstack(self.norm_alphas).T
        combo = A.dot(np.array(self.weights))
        target = self._load_validation_target()
        return information_coefficient(combo, target)

    def _load_validation_target(self) -> np.ndarray:
        """
        获取注入的目标序列数组（未来收益或方向）。

        Returns:
            np.ndarray: 目标序列数值数组。

        Raises:
            AttributeError: 若未调用 `inject_data` 注入数据时。
        """
        if not hasattr(self, "_target"):
            raise AttributeError("请先调用 inject_data() 注入行情和目标序列")
        return self._target

    def _compute_alpha_from_expr(self, expr: str) -> np.ndarray:
        """
        解析逆波兰表达式（RPN），执行算子运算，生成原始因子序列。

        Args:
            expr (str): 形如 "close 5 ts_mean" 的 RPN 表达式字符串。

        Returns:
            np.ndarray: 计算得到的因子值数组，dtype=float64。

        Raises:
            AttributeError: 若未注入 `data` 时调用。
            ValueError: 表达式格式错误（未知 token、参数不足或最终栈深 != 1）。
        """
        if not hasattr(self, "data"):
            raise AttributeError(
                "AlphaCombinationModel 需先注入行情 DataFrame 到 self.data"
            )

        tokens: List[str] = expr.strip().split()
        stack: List[Union[pd.Series, float]] = []

        # 预构建 “函数名 → (Callable, arity)” 映射
        func_map: Dict[str, Tuple[Callable, int]] = {}
        for name, fn in inspect.getmembers(operators, inspect.isfunction):
            sig = inspect.signature(fn)
            func_map[name] = (fn, len(sig.parameters))

        for tk in tokens:
            # -- 1. 基础变量 ----------------------------------------------------
            if tk in self.data.columns:
                stack.append(self.data[tk])
            # -- 2. 数值常量 ----------------------------------------------------
            elif _is_float(tk):
                val = float(tk)
                if val.is_integer():
                    stack.append(int(val))
                else:
                    stack.append(float(tk))
            # -- 3. 函数 / 运算符 ----------------------------------------------
            elif tk in func_map:
                fn, arity = func_map[tk]
                if len(stack) < arity:
                    raise ValueError(f"RPN 表达式参数不足：{tk}")
                # 注意：弹栈顺序需反转以保持原来顺序
                args = [stack.pop() for _ in range(arity)][::-1]
                res = fn(*args)
                stack.append(res)
            # -- 4. 支持最常见的四则运算符 -------------------------------------
            elif tk in {"+", "-", "*", "/"}:
                if len(stack) < 2:
                    raise ValueError(f"RPN 表达式参数不足：{tk}")
                b, a = stack.pop(), stack.pop()
                if tk == "+": res = a + b
                elif tk == "-": res = a - b
                elif tk == "*": res = a * b
                elif tk == "/": res = a / (b.replace(0, np.nan) if isinstance(b, pd.Series) else (b if b != 0 else np.nan))
                stack.append(res)
            else:
                raise ValueError(f"未知 token：{tk}")

        if len(stack) != 1:
            raise ValueError("RPN 表达式最终栈深度应为 1")
        # 返回 numpy 数组，后续会做 winsorize + z-score
        series = stack[0]
        return np.asarray(series.values, dtype=np.float64)
    
    def _maybe_normalize(self, alpha: np.ndarray) -> np.ndarray:
        """
        对原始因子序列执行截尾（winsorize）和 Z-score 标准化。

        Args:
            alpha (np.ndarray): 原始因子值数组。

        Returns:
            np.ndarray: 归一化后的因子序列。
        """
        return winsorize(zscore_normalize(alpha))

        # === 1. RPN 解析与执行 ====================================================

def _is_float(str) -> bool:
    """
    判断字符串是否可转换为浮点数。

    Args:
        s (str): 待检测字符串。

    Returns:
        bool: 若能安全转换为 float，则返回 True，否则 False。
    """
    try:
        float(str)
        return True
    except ValueError:
        return False

================================================================================
File: ./tokenizer.py
--------------------------------------------------------------------------------
# tokenizer.py
import re
import inspect
import operators
from typing import List, Dict

class AlphaTokenizer:
    """
    将逆波兰表达式 (RPN) 与 token 序列相互映射的分词器。

    - **词表**：基础行情字段、常量桶、所有算子以及四则运算符  
    - **特殊标记**：`[PAD]` 填充、`[BOS]` 序列起始、`[SEP]` 序列终止
    """
    def __init__(self, base_fields: List[str] = None, const_buckets: List[float] = None):
        """
        构造分词器并自动扫描 `operators.py` 生成完整词表。

        参数
        ----------
        base_fields : list[str], 可选
            基础行情字段名称列表，默认为  
            ``['open', 'high', 'low', 'close', 'volume']``。
        const_buckets : list[float], 可选
            预定义浮点常量桶，默认为  
            ``[1, 3, 5, 10, 20, 0.1, 0.5]``。
        """
        if base_fields is None:
            base_fields = ['open', 'high', 'low', 'close', 'volume']
        if const_buckets is None:
            const_buckets = [1, 3, 5, 10, 20, 0.1, 0.5]

        self.base_fields = base_fields
        self.const_buckets = const_buckets

        # 特殊 token
        self.special_tokens = ['[PAD]', '[BOS]', '[SEP]']

        # 基础字段 token
        field_tokens = base_fields

        # 常量 token
        const_tokens = [f'CONST_{c}' for c in const_buckets]

        # 算子 token：自动扫描 operators.py
        op_tokens = []
        for name, fn in inspect.getmembers(operators, inspect.isfunction):
            if not name.startswith("_"):
                op_tokens.append(name)

        # 词表
        arith = ['+', '-', '*', '/']
        self.vocab: List[str] = self.special_tokens + field_tokens + const_tokens + op_tokens + arith

        # ID ↔ Token 映射
        self.token_to_id: Dict[str, int] = {tok: idx for idx, tok in enumerate(self.vocab)}
        self.id_to_token: Dict[int, str] = {idx: tok for tok, idx in self.token_to_id.items()}

        # 便捷属性
        self.pad_token_id = self.token_to_id['[PAD]']
        self.bos_token_id = self.token_to_id['[BOS]']
        self.sep_token_id = self.token_to_id['[SEP]']

    @property
    def vocab_size(self) -> int:
        """
        返回当前词表大小。

        返回
        ----------
        int
            ``len(self.vocab)``。
        """
        return len(self.vocab)

    def encode(self, expr: str, add_special_tokens: bool = True) -> List[int]:
        """
        将 RPN 字符串编码为 token ID 序列。

        参数
        ----------
        expr : str
            形如 ``"close 5 ts_mean"`` 的逆波兰表达式。
        add_special_tokens : bool, 可选
            是否在首尾分别加入 `[BOS]` 与 `[SEP]`，默认为 ``True``。

        返回
        ----------
        list[int]
            token ID 序列。
        """
        tokens = expr.strip().split()
        ids = []
        for tk in tokens:
            # 基础字段
            if tk in self.token_to_id:
                ids.append(self.token_to_id[tk])
            # 常量：动态加入最近的 bucket
            elif self._is_float(tk):
                val = float(tk)
                const_tok = self._map_to_const(val)
                ids.append(self.token_to_id[const_tok])
            else:
                raise ValueError(f"未知 token：{tk}")

        if add_special_tokens:
            ids = [self.bos_token_id] + ids + [self.sep_token_id]
        return ids

    def decode(self, ids: List[int], remove_special_tokens: bool = True) -> str:
        """
        将 token ID 序列还原为 RPN 字符串。

        参数
        ----------
        ids : list[int]
            编码后的 token ID 列表。
        remove_special_tokens : bool, 可选
            是否去除 `[BOS]` / `[SEP]` / `[PAD]`，默认为 ``True``。

        返回
        ----------
        str
            逆波兰表达式，常量会被还原为原始数字字符串。
        """
        toks = []
        for idx in ids:
            if remove_special_tokens and idx in [self.bos_token_id, self.sep_token_id, self.pad_token_id]:
                continue
            toks.append(self.id_to_token.get(idx, 'UNK'))
        # 常量还原
        toks = [self._const_to_str(tk) for tk in toks]
        return " ".join(toks)

    def _map_to_const(self, val: float) -> str:
        """
        将任意浮点数映射到最近的常量桶，并返回其 token 名。

        参数
        ----------
        val : float
            原始浮点数。

        返回
        ----------
        str
            形如 ``"CONST_5"`` 的 token 名称。
        """
        closest = min(self.const_buckets, key=lambda x: abs(x - val))
        return f'CONST_{closest}'

    def _const_to_str(self, token: str) -> str:
        """
        将常量 token 名恢复为数字字符串。

        参数
        ----------
        token : str
            形如 ``"CONST_3"`` 的 token。

        返回
        ----------
        str
            ``"3"``；如果传入非常量 token，则原样返回。
        """
        if token.startswith('CONST_'):
            return token.split('_', 1)[1]
        return token

    @staticmethod
    def _is_float(s: str) -> bool:
        """
        判断字符串能否安全转换为 `float`。

        参数
        ----------
        s : str
            待检测字符串。

        返回
        ----------
        bool
            可转换返回 ``True``，否则 ``False``。
        """
        try:
            float(s)
            return True
        except ValueError:
            return False



================================================================================
File: ./generator.py
--------------------------------------------------------------------------------
from typing import List, Tuple, Dict, Any
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from torch.distributions import Categorical
from envs import AlphaGenerationEnv


class PolicyNetwork(nn.Module):
    def __init__(self, vocab_size: int, hidden_dim: int, num_layers: int = 1) -> None:
        """
        策略网络，用于生成下一个 token 的概率分布。

        Args:
            vocab_size: token vocabulary 大小，即动作空间大小。
            hidden_dim: LSTM 隐藏层维度。
            num_layers: LSTM 层数。
        """
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)
        self.fc_logits = nn.Linear(hidden_dim, vocab_size)

    def forward(
        self, x: torch.Tensor, hidden: Tuple[torch.Tensor, torch.Tensor]
    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        Args:
            x: 输入 token 序列，形状为 (batch_size, seq_len)。
            hidden: LSTM 的隐藏状态 (h, c)，形状为 (num_layers, batch_size, hidden_dim)。

        Returns:
            logits: 下一个 token 的 logits，形状为 (batch_size, vocab_size)。
            hidden: 更新后的隐藏状态。
        """
        emb = self.embedding(x)
        out, hidden = self.lstm(emb, hidden)
        logits = self.fc_logits(out[:, -1, :])
        return logits, hidden

    def init_hidden(
        self, batch_size: int, device: torch.device
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        初始化隐藏状态为全零。

        Args:
            batch_size: 批量大小。
            device: 所在设备（cpu 或 cuda）。

        Returns:
            初始化的 (h, c) 状态。
        """
        num_layers = self.lstm.num_layers
        hidden_dim = self.lstm.hidden_size
        h0 = torch.zeros(num_layers, batch_size, hidden_dim, device=device)
        c0 = torch.zeros(num_layers, batch_size, hidden_dim, device=device)
        return (h0, c0)


class ValueNetwork(nn.Module):
    def __init__(self, vocab_size: int, hidden_dim: int, num_layers: int = 1) -> None:
        """
        价值网络，用于估计当前 token 序列的状态价值 V(s)

        Args:
            vocab_size: token vocabulary 大小。
            hidden_dim: LSTM 隐藏层维度。
            num_layers: LSTM 层数。
        """
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)
        self.fc_value = nn.Linear(hidden_dim, 1)

    def forward(
        self, x: torch.Tensor, hidden: Tuple[torch.Tensor, torch.Tensor]
    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        参数:
            x: 输入 token 序列，形状为 (batch_size, seq_len)。
            hidden: LSTM 隐藏状态。

        返回值:
            value: 每个序列的状态价值，形状为 (batch_size,)。
            hidden: 更新后的隐藏状态。
        """
        emb = self.embedding(x)
        out, hidden = self.lstm(emb, hidden)
        value = self.fc_value(out[:, -1, :]).squeeze(-1)
        return value, hidden

    def init_hidden(
        self, batch_size: int, device: torch.device
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        同 PolicyNetwork，初始化 LSTM 隐藏状态。
        """
        num_layers = self.lstm.num_layers
        hidden_dim = self.lstm.hidden_size
        h0 = torch.zeros(num_layers, batch_size, hidden_dim, device=device)
        c0 = torch.zeros(num_layers, batch_size, hidden_dim, device=device)
        return (h0, c0)


class RLAlphaGenerator:
    """
    使用 PPO 在 `AlphaGenerationEnv` 中训练策略网络，自动生成高 IC 的 Alpha 表达式。
    """

    def __init__(self, env: AlphaGenerationEnv, config: Dict[str, Any]) -> None:
        """
        参数:
            env: 强化学习环境，需支持 reset(), step(action), valid_actions() 接口。
            config: 包含网络和 PPO 超参数的配置字典。
        """
        self.env = env
        self.vocab_size = config["vocab_size"]
        self.hidden_dim = config.get("hidden_dim", 128)
        self.device = config.get("device", "cpu")

        self.policy_net = PolicyNetwork(self.vocab_size, self.hidden_dim).to(
            self.device
        )
        self.value_net = ValueNetwork(self.vocab_size, self.hidden_dim).to(self.device)

        self.policy_optimizer = torch.optim.AdamW(
            self.policy_net.parameters(), lr=config.get("lr_policy", 3e-4)
        )
        self.value_optimizer = torch.optim.AdamW(
            self.value_net.parameters(), lr=config.get("lr_value", 1e-3)
        )

        self.gamma = config.get("gamma", 1.0)
        self.clip_eps = config.get("clip_eps", 0.2)
        self.entropy_coef = config.get("entropy_coef", 0.01)
        self.value_coef = config.get("value_coef", 0.5)
        self.update_epochs = config.get("update_epochs", 4)
        self.batch_size = config.get("batch_size", 64)
        self.max_seq_len = config.get("max_seq_len", 20)
    
    def train(self, num_iterations: int) -> None:
        """
        用 PPO 训练策略 & 价值网络。
        每轮：
          ① 与环境交互，采样 batch_size 步（或更多）完整 episode
          ② 计算优势 (A = R - V) 并标准化
          ③ 对策略 / 价值网络做多次 epoch 更新
        """
        for it in range(1, num_iterations + 1):
            # ------ 采样轨迹 -------------------------------------------------
            states, actions, old_logps, returns, advantages = self._collect_trajectories()

            # Advantage 标准化以稳定训练
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

            # 打包成 DataLoader，方便多轮 epoch shuffle
            ds = TensorDataset(states, actions, old_logps, returns, advantages)
            loader = DataLoader(ds, batch_size=self.batch_size, shuffle=True)

            for _ in range(self.update_epochs):
                for s, a, logp_old, ret, adv in loader:
                    s = s.to(self.device)
                    a = a.to(self.device)
                    logp_old = logp_old.to(self.device).detach()
                    ret = ret.to(self.device)
                    adv = adv.to(self.device)

                    # ----- 1. 重新计算策略 & log π(a|s) ---------------------
                    h0_p = self.policy_net.init_hidden(s.size(0), self.device)
                    logits, _ = self.policy_net(s, h0_p)
                    dist = Categorical(logits=logits)
                    logp = dist.log_prob(a)
                    entropy = dist.entropy().mean()

                    # ----- 2. 计算 PPO clip 损失 ---------------------------
                    ratio = (logp - logp_old).exp()
                    pg_loss = -torch.min(
                        ratio * adv,
                        torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * adv,
                    ).mean()

                    # ----- 3. 计算价值函数损失 -----------------------------
                    h0_v = self.value_net.init_hidden(s.size(0), self.device)
                    value_pred, _ = self.value_net(s, h0_v)
                    value_loss = F.mse_loss(value_pred.squeeze(-1), ret)

                    # ----- 4. 总损失 & 反向传播 ----------------------------
                    loss = pg_loss + self.value_coef * value_loss - self.entropy_coef * entropy

                    self.policy_optimizer.zero_grad()
                    self.value_optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
                    torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 1.0)
                    self.policy_optimizer.step()
                    self.value_optimizer.step()

            # ------ 打印监控信息 --------------------------------------------
            if it % 10 == 0:
                with torch.no_grad():
                    avg_ret = returns.mean().item()
                    combo_ic = self.env.combo_model.score()
                print(f"[Iter {it:04d}]  AvgReturn={avg_ret:+.4f}   ComboIC={combo_ic:+.4f}")

    def _collect_trajectories(
        self,
    ) -> Tuple[
        List[torch.Tensor], List[int], List[torch.Tensor], List[float], List[float]
    ]:
        """
        从环境中采样一批完整轨迹，并计算回报与优势值（GAE λ=1）。

        返回
        ----------
        states : torch.Tensor
            形状 ``(T, seq_len)`` 的 token 序列张量。
        actions : torch.Tensor
            长度 ``T`` 的动作 ID。
        logps : torch.Tensor
            对应动作的对数概率。
        returns : torch.Tensor
            折现回报。
        advantages : torch.Tensor
            Advantage 值（此处为 `return - value`）。
        """
        states, actions, logps, rewards, dones, values = [], [], [], [], [], []

        obs = torch.tensor([self.env.reset()], device=self.device)
        h_p, h_v = self.policy_net.init_hidden(1, self.device), self.value_net.init_hidden(1, self.device)

        while len(states) < self.batch_size:
            logits, h_p = self.policy_net(obs, h_p)

            # -------- Invalid-action-mask --------
            valid = self.env.valid_actions()
            mask = torch.full((self.vocab_size,), float('-inf'), device=self.device)
            mask[valid] = 0.0                                # 合法动作设置成 0，其它仍为 −inf
            logits = logits + mask                          # 非法动作 logits 变为 −inf
            dist = Categorical(logits=logits)
            
            action = dist.sample()
            logp = dist.log_prob(action)

            value, h_v = self.value_net(obs, h_v)

            next_obs, reward, done, _ = self.env.step(action.item())
            states.append(obs.squeeze(0))
            actions.append(action)
            logps.append(logp)
            rewards.append(torch.tensor(reward, device=self.device, dtype=torch.float32))
            dones.append(done)
            values.append(value.squeeze(0))

            if done:
                obs = torch.tensor([self.env.reset()], device=self.device)
                h_p, h_v = self.policy_net.init_hidden(1, self.device), self.value_net.init_hidden(1, self.device)
            else:
                obs = torch.tensor([next_obs], device=self.device)
        
        # ===== 计算 GAE(λ=1) → advantage = return - value =====
        returns, advantages, R = [], [], torch.tensor(0.0, device=self.device)
        for r, v, d in zip(reversed(rewards), reversed(values), reversed(dones)):
            R = r + self.gamma * R * (1.0 - float(d))
            returns.insert(0, R)
            advantages.insert(0, R - v)
        
        return (
            torch.stack(states),
            torch.stack(actions).squeeze(-1),
            torch.stack(logps),
            torch.stack(returns),
            torch.stack(advantages),
        )
    
    

================================================================================
File: ./paradigm.py
--------------------------------------------------------------------------------
import numpy as np

# alpha_mining_framework/
# ├── __init__.py
# ├── data.py            # Data loading & preprocessing
# ├── utils.py           # Metrics: IC, RankIC, normalization, caching
# ├── combination.py     # AlphaCombinationModel: linear combine + weight optimization
# ├── env.py             # AlphaGenerationEnv: custom RL environment without gym
# ├── generator.py       # RLAlphaGenerator: PPO policy & value networks
# ├── train.py           # Main training loop implementing Algorithm 2
# └── run_backtest.py    # Backtesting & investment simulation (Figure 5)






================================================================================
File: ./train.py
--------------------------------------------------------------------------------
# train.py
from combination import AlphaCombinationModel
from envs import AlphaGenerationEnv
from generator import RLAlphaGenerator

if __name__ == "__main__":
    # 1. Load data
    # 2. Initialize AlphaCombinationModel
    # 3. Initialize tokenizer 和 AlphaGenerationEnv
    # 4. Initialize RLAlphaGenerator
    # 5. 调用 generator.train()
    # 6. 保存发现的 alphas 和权重
    pass


================================================================================
File: ./operators.py
--------------------------------------------------------------------------------
import pandas as pd
import numpy as np

# -----------------------------
# Time-Series Operators (TS)
# -----------------------------

def ref(series: pd.Series, period: int) -> pd.Series:
    """Ref(x, t)：滞后期 t 的值"""
    return series.shift(period)

def ts_mean(series: pd.Series, window: int) -> pd.Series:
    """Mean(x, t)：滚动均值"""
    return series.rolling(window).mean()

def ts_med(series: pd.Series, window: int) -> pd.Series:
    """Med(x, t)：滚动中位数"""
    return series.rolling(window).median()

def ts_sum(series: pd.Series, window: int) -> pd.Series:
    """Sum(x, t)：滚动求和"""
    return series.rolling(window).sum()

def ts_std(series: pd.Series, window: int) -> pd.Series:
    """Std(x, t)：滚动标准差"""
    return series.rolling(window).std()

def ts_var(series: pd.Series, window: int) -> pd.Series:
    """Var(x, t)：滚动方差"""
    return series.rolling(window).var()

def ts_max(series: pd.Series, window: int) -> pd.Series:
    """Max(x, t)：滚动最大值"""
    return series.rolling(window).max()

def ts_min(series: pd.Series, window: int) -> pd.Series:
    """Min(x, t)：滚动最小值"""
    return series.rolling(window).min()

def ts_mad(series: pd.Series, window: int) -> pd.Series:
    """Mad(x, t)：滚动平均绝对偏差"""
    return series.rolling(window).apply(
        lambda x: np.mean(np.abs(x - np.mean(x))), raw=True
    )

def ts_delta(series: pd.Series, period: int = 1) -> pd.Series:
    """Delta(x, t)：与 t 期前的差值"""
    return series.diff(period)

def ts_rank(series: pd.Series, window: int) -> pd.Series:
    """滚动排序百分位"""
    def _rank(x):
        return pd.Series(x).rank(pct=True).iloc[-1]
    return series.rolling(window).apply(_rank, raw=True)

def ts_corr(x: pd.Series, y: pd.Series, window: int) -> pd.Series:
    """Corr(x, y, t)：滚动皮尔森相关系数"""
    return x.rolling(window).corr(y)

def ts_cov(x: pd.Series, y: pd.Series, window: int) -> pd.Series:
    """Cov(x, y, t)：滚动协方差"""
    return x.rolling(window).cov(y)

def ts_wma(series: pd.Series, window: int) -> pd.Series:
    """WMA(x, t)：滚动线性加权平均"""
    weights = np.arange(1, window + 1)
    return series.rolling(window).apply(
        lambda x: np.dot(x, weights) / weights.sum(), raw=True
    )

def ts_ema(series: pd.Series, span: int) -> pd.Series:
    """EMA(x, t)：指数移动平均（span 可理解为 t）"""
    return series.ewm(span=span, adjust=False).mean()


# -----------------------------
# Utility Operators
# -----------------------------

def decay_linear(series: pd.Series, window: int) -> pd.Series:
    """线性衰减加权平均（同 WMA）"""
    weights = np.arange(1, window + 1)[::-1]
    return series.rolling(window).apply(
        lambda x: np.dot(x, weights) / weights.sum(), raw=True
    )

def ts_zscore(series: pd.Series, window: int) -> pd.Series:
    """时序标准分"""
    return (series - ts_mean(series, window)) / ts_std(series, window)

def ts_return(series: pd.Series, period: int = 1) -> pd.Series:
    """周期收益率"""
    return series.pct_change(period)




================================================================================
File: ./data.py
--------------------------------------------------------------------------------
# data.py 

import pandas as pd
import numpy as np


def load_market_data(path: str = "data/rb_20250606_primary.csv", multiplier: int = 10, n: int = 5):
    """
    读取并预处理原始 A 股行情数据。

    参数
    ----------
    path : str, 可选
        CSV 文件的本地路径，默认为 ``"data/rb_20250606_primary.csv"``。
    multiplier: int
    n: int 可选
        预测 n 秒之后的收益率

    返回
    ----------
    pandas.DataFrame
        处理后的行情特征表，包含基础字段以及 ``target``（20 日远期收益）列。
    """
    # TODO: 这里暂时读取本地文件

    df = pd.read_csv(path, parse_dates=["timestamp"], index_col="timestamp")

    df = df.sort_index()

    df['d_vol'] = df['volume'].diff()
    df['d_amt'] = df['amount'].diff()
    df['d_oi']  = df['openInterest'].diff()

    df.loc[df['d_vol'] <= 0, ['d_vol', 'd_amt']] = np.nan

    df['trade_price'] = df['d_amt'] / df['d_vol'] / multiplier

    one_sec = df.resample('1s')

    ohlc = pd.DataFrame({
        'open' : one_sec['last'].first(),
        'high' : one_sec['trade_price'].max(),
        'low'  : one_sec['trade_price'].min(),
        'close': one_sec['last'].last(),
        'volume': one_sec['d_vol'].sum(),        # 可选：这一秒真正的成交量
        'amount': one_sec['d_amt'].sum(),
        'openInterest': one_sec['d_oi'].sum(),
    })

    mask = ohlc['high'].isna()
    ohlc.loc[mask, 'high'] = one_sec['last'].max()[mask]
    ohlc.loc[mask, 'low']  = one_sec['last'].min()[mask]

    ohlc['target'] = ohlc['close'].pct_change(periods=-n, fill_method=None)
    return ohlc


