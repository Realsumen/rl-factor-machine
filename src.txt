================================================================================
File: ./unit_test.py
--------------------------------------------------------------------------------
# %% [markdown]
# ### 这个 notebook 用来读取文件，并测试一些方法

# %%
import os
import numpy as np


def get_all_py_files(root_dir: str) -> list:
    """
    获取指定目录及其子目录下的所有 .py 文件路径。

    Args:
        root_dir (str): 起始目录路径。

    Returns:
        list: 包含所有 .py 文件完整路径的列表。
    """
    py_files = []
    for dirpath, _, filenames in os.walk(root_dir):
        for file in filenames:
            if file.endswith(".py"):
                py_files.append(os.path.join(dirpath, file))
    return py_files


def write_all_py_contents_to_output(
    py_files: list, output_path: str = "src.txt"
) -> None:
    """
    将所有 .py 文件的内容写入一个文本文件中，并打印文件名作为分隔。

    Args:
        py_files (list): .py 文件路径列表。
        output_path (str): 输出文件路径。
    """
    with open(output_path, "w", encoding="utf-8") as out_file:
        for path in py_files:
            out_file.write(f"{'=' * 80}\n")
            out_file.write(f"File: {path}\n")
            out_file.write(f"{'-' * 80}\n")
            try:
                with open(path, "r", encoding="utf-8") as f:
                    out_file.write(f.read())
            except Exception as e:
                out_file.write(f"⚠️ Error reading {path}: {e}\n")
            out_file.write("\n\n")


if True:
    root_directory = "."  # 当前目录
    all_py_files = get_all_py_files(root_directory)
    write_all_py_contents_to_output(all_py_files)
    print(f"📄 所有 Python 文件内容已写入 src.txt（共 {len(all_py_files)} 个文件）")


# %%
import importlib
from utility import set_random_seed


def reload_components():
    """
    重新加载以下模块，以便在开发过程中即时生效：
      - data.load_market_data
      - tokenizer.AlphaTokenizer
      - combination.AlphaCombinationModel
      - envs.AlphaGenerationEnv
      - generator.RLAlphaGenerator
    """
    import data, tokenizer, combination, alpha_generation_env, generator

    importlib.reload(data)
    importlib.reload(tokenizer)
    importlib.reload(combination)
    importlib.reload(alpha_generation_env)
    importlib.reload(generator)

    # 重新绑定到本地名称（可选）
    from data import load_market_data
    from tokenizer import AlphaTokenizer
    from combination import AlphaCombinationModel
    from alpha_generation_env import AlphaGenerationEnv
    from generator import RLAlphaGenerator

    return {
        "load_market_data": load_market_data,
        "AlphaTokenizer": AlphaTokenizer,
        "AlphaCombinationModel": AlphaCombinationModel,
        "AlphaGenerationEnv": AlphaGenerationEnv,
        "RLAlphaGenerator": RLAlphaGenerator,
    }


components = reload_components()


# %% [markdown]
# ### 1.测试 AlphaCombinationModel._compute_alpha_from_expr

# %%
from combination import AlphaCombinationModel
from data import load_market_data

df = load_market_data()
model = AlphaCombinationModel()
model.inject_data(df, target_col="target")

# 表达式：close 的 100 秒均值
expr = "close 100 ts_mean"
alpha = model._compute_alpha_from_expr(expr)

print("alpha shape:", alpha.shape)
print("alpha sample:", alpha[~np.isnan(alpha)][:5])


# %% [markdown]
# ###  2. 测试 AlphaCombinationModel.add_alpha_expr

# %%
ic = model.add_alpha_expr("high low sub 100 ts_max")
print("该因子的 IC 为：", ic)
print("当前池中因子数：", len(model.alphas))


# %% [markdown]
# ### 3. 测试 AlphaTokenizer.encode / decode

# %%
from tokenizer import AlphaTokenizer

tokenizer = AlphaTokenizer()

expr = "close 5 ts_mean"
ids = tokenizer.encode(expr)
decoded = tokenizer.decode(ids)

print("Token IDs:", ids)
print("Decoded expr:", decoded)


# %% [markdown]
# ### 4. 测试 AlphaGenerationEnv.reset / step

# %%
from alpha_generation_env import AlphaGenerationEnv
from combination import AlphaCombinationModel
from tokenizer import AlphaTokenizer
from data import load_market_data

df = load_market_data()
combo = AlphaCombinationModel()
combo.inject_data(df, target_col="target")
tokenizer = AlphaTokenizer()
env = AlphaGenerationEnv(combo, tokenizer)

obs = env.reset()
print("初始状态 token IDs:", obs)

valid = env.valid_actions()
action = valid[1]
obs2, reward, done, info = env.step(action)
print("新状态:", obs2)
print("Reward:", reward, "Done:", done)


# %% [markdown]
# ### 5. 测试 PolicyNetwork / ValueNetwork 输出维度

# %%
import torch
from generator import PolicyNetwork, ValueNetwork

vocab_size = 50
seq_len = 6
hidden_dim = 64
device = "cpu"

x = torch.randint(0, vocab_size, (1, seq_len))  # batch_size=1
policy = PolicyNetwork(vocab_size, hidden_dim).to(device)
value = ValueNetwork(vocab_size, hidden_dim).to(device)

h0_p = policy.init_hidden(1, device)
logits, _ = policy(x, h0_p)
print("Policy logits shape:", logits.shape)  # 应为 (1, vocab_size)

h0_v = value.init_hidden(1, device)
v, _ = value(x, h0_v)
print("Value estimate shape:", v.shape)  # 应为 (1,)


# %% [markdown]
# ### 6. 测试 RLAlphaGenerator._collect_trajectories

# %%
from generator import RLAlphaGenerator
from alpha_generation_env import AlphaGenerationEnv
from combination import AlphaCombinationModel
from tokenizer import AlphaTokenizer
from data import load_market_data
from utility import set_random_seed

# reload_components()

set_random_seed(10)

df = load_market_data()
combo = AlphaCombinationModel()
combo.inject_data(df, "target")
tokenizer = AlphaTokenizer()
env = AlphaGenerationEnv(combo, tokenizer, max_len=20)

cfg = dict(
    vocab_size=tokenizer.vocab_size,
    hidden_dim=64,
    batch_size=1280,
    device="cpu",
)

agent = RLAlphaGenerator(env, cfg)

s, a, logp, ret, adv = agent._collect_trajectories()
print("Sample states shape:", s.shape)
print("Sample actions shape:", a.shape)
print("Sample rewards (returns):", ret.shape, ret)


# %%


================================================================================
File: ./alpha_generation_env.py
--------------------------------------------------------------------------------
# alpha_generation_env.py
from combination import AlphaCombinationModel
from tokenizer import AlphaTokenizer
from typing import List
from operators import FUNC_MAP


class AlphaGenerationEnv:
    """
    自定义强化学习环境：生成逆波兰表达式（RPN）的序列决策过程。

    - **状态**：当前已生成的 token ID 列表
    - **动作**：下一个 token 的 ID
    - **奖励**：组合模型评估的单因子 IC
    - **终止条件**：生成 `[SEP]` 或达到 `max_len`
    """

    def __init__(
        self, combo_model: AlphaCombinationModel, tokenizer: AlphaTokenizer, max_len=20
    ):
        """
        初始化环境。

        参数
        ----------
        combo_model : AlphaCombinationModel
            负责因子池管理与 IC 计算的组合模型。
        tokenizer : AlphaTokenizer
            RPN ↔ token 序列转换器。
        max_len : int, 可选
            生成序列的最⼤长度（含 `[BOS]` 与 `[SEP]`），默认 20。
        """
        self.combo_model = combo_model
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.reset()

    def reset(self):
        """
        重新开始一条新序列。

        返回
        ----------
        List[int]
            仅包含 `[BOS]` 的初始序列。
        """
        self.sequence: List[int] = [self.tokenizer.bos_token_id]
        self.done: bool = False
        self._stack_types: List[str] = []
        return self._get_obs()

    def step(self, action: int):
        """
        执行一步生成并返回环境转移结果。

        参数
        ----------
        action : int
            选定的 token ID。

        返回
        ----------
        obs : List[int]
            新的 token 序列。
        reward : float
            若已结束则为该表达式的单因子 IC，否则为 0。
        done : bool
            是否到达终止状态。
        info : dict
            预留调试信息，当前为空字典。
        """

        # —— 记录前一时刻的 info ——
        info = {
            "prev_stack_types": self._stack_types.copy(),
            "prev_depth": len(self._stack_types),
            "prev_valid_actions": self.valid_actions(),
            "action_id": action,
            "action_token": self.tokenizer.id_to_token[action],
        }

        # —— 执行动作：更新 sequence & stack_types ——
        self.sequence.append(action)
        tok = info["action_token"]
        if tok in self.tokenizer.operand_type_map:
            self._stack_types.append(self.tokenizer.operand_type_map[tok])
        elif tok in FUNC_MAP:
            _, arity, _ = FUNC_MAP[tok]
            for _ in range(arity):
                self._stack_types.pop()
            self._stack_types.append("Series")
        else:
            pass

        # —— 再补充依赖于“后”状态的 info ——
        info.update(
            {
                "new_stack_types": self._stack_types.copy(),
                "new_depth": len(self._stack_types),
                "remaining": self.max_len - len(self.sequence),
            }
        )

        # —— 计算 reward & done ——
        reward = 0.0
        if action == self.tokenizer.sep_token_id or len(self.sequence) >= self.max_len:
            expr = self.tokenizer.decode(self.sequence, remove_special_tokens=True)
            try:
                reward = self.combo_model.evaluate_alpha(expr)
            except ValueError as e:
                reward = -1.0
                info["error"] = str(e)
            self.done = True

        obs = self._get_obs()
        return obs, reward, self.done, info

    def _get_obs(self):
        """
        获取当前观测值（token 序列）。

        返回
        ----------
        List[int]
            当前生成序列（含 `[BOS]`，可能含 `[SEP]`）。
        """
        return self.sequence

    def valid_actions(self) -> List[int]:
        """
        获取当前状态下所有合法动作的 token ID。

        根据当前表达式的栈状态与剩余步数，筛选出能够构成
        合法逆波兰表达式的下一个 token。

        包含如下合法性检查：
        - 栈深是否满足操作符的参数需求；
        - 栈顶参数类型是否与操作符定义匹配；
        - 剩余步数是否足够完成表达式闭合；
        - 对于操作数，保证不会超出最大长度限制。

        返回
        ----------
        List[int]
            所有合法 token 的 ID 列表。
        """
        depth = len(self._stack_types)
        remaining = self.max_len - len(self.sequence)

        if remaining == 1:
            return [self.tokenizer.sep_token_id] if depth == 1 else []

        valid = []
        
        if depth == 1:
            valid.append(self.tokenizer.sep_token_id)

        for tok_id, tok in self.tokenizer.id_to_token.items():
            if tok in ("[PAD]", "[BOS]", "[SEP]"):
                continue

            # 时间序列算子前，只有整型常量才合法
            if tok in FUNC_MAP and tok.startswith("ts_"):
                last_tok = self.tokenizer.id_to_token[self.sequence[-1]]
                last_type = self.tokenizer.operand_type_map.get(last_tok)
                if last_type != "Scalar_INT":
                    continue
                if last_tok == "CONST_1":
                    continue
                if tok == "ts_kurt" and last_tok == "CONST_3":
                    continue

            if tok in FUNC_MAP:
                fn, arity, param_types = FUNC_MAP[tok]
                if depth < arity:
                    continue
                if not all(
                    _type_compatible(g, r)
                    for g, r in zip(self._stack_types[-arity:], param_types)
                ):
                    continue
                if depth - arity + 1 >= remaining:
                    continue
                valid.append(tok_id)
            else:
                # 常量不能连着常量
                if depth > 0 \
                and self.tokenizer.operand_type_map[tok].startswith("Scalar") \
                and self._stack_types[-1].startswith("Scalar"):
                    continue
                if depth + 1 < remaining:
                    valid.append(tok_id)
        return valid


def _type_compatible(given: str, required: str) -> bool:
    if given == required:
        return True
    if required == "Any":
        return True
    if given == "Scalar_INT" and required == "Scalar_FLOAT":
        return True
    return False


================================================================================
File: ./utility.py
--------------------------------------------------------------------------------
# utility.py
import numpy as np
import pandas as pd
import os
import random
import torch


def zscore_normalize(alpha: np.ndarray) -> pd.Series:
    """
    对因子序列进行Z-score标准化。

    该函数对输入数组进行去均值除以标准差处理，
    并将结果限制为有限值，所有NaN或无穷值替换为0。

    Args:
        alpha (np.ndarray): 原始因子值数组，dtype可包含NaN。

    Returns:
        pd.Series: 标准化后的序列，长度与输入相同，无NaN。
    """
    a = np.asarray(alpha, dtype=np.float64)
    mean = np.nanmean(a)
    std = np.nanstd(a)
    if std == 0 or np.isnan(std):
        return pd.Series(np.zeros_like(a))
    with np.errstate(invalid="ignore", divide="ignore"):
        z = (a - mean) / std
    z = np.nan_to_num(z, nan=0.0, posinf=0.0, neginf=0.0)
    return pd.Series(z)


def winsorize(
    alpha: np.ndarray, lower_quantile: float = 0.01, upper_quantile: float = 0.99
) -> pd.Series:
    """
    对因子序列进行截尾处理，限制极端值。

    使用指定上下分位数计算阈值，并将超出范围的值裁剪到边界，
    所有NaN或无穷值替换为对应边界值。

    Args:
        alpha (np.ndarray): 原始因子值数组。
        lower_quantile (float): 下分位点，默认0.01。
        upper_quantile (float): 上分位点，默认0.99。

    Returns:
        pd.Series: 截尾后的序列，长度与输入相同，无NaN。
    """
    a = np.asarray(alpha, dtype=float)
    with np.errstate(invalid="ignore", over="ignore"):
        lower = np.nanpercentile(a, lower_quantile * 100)
        upper = np.nanpercentile(a, upper_quantile * 100)
        clipped = np.clip(a, lower, upper)
    clipped = np.nan_to_num(clipped, nan=lower, posinf=upper, neginf=lower)
    return pd.Series(clipped)


def information_coefficient(factor: np.ndarray, target: np.ndarray) -> float:
    """
    计算因子与目标的Pearson相关系数（信息系数）。

    若输入长度不匹配或有效样本少于2，返回0；
    若任意方差为0，返回0。

    Args:
        factor (np.ndarray): 因子值数组。
        target (np.ndarray): 目标值数组（未来收益）。

    Returns:
        float: Pearson相关系数，范围[-1,1]，或0表示无效。
    """
    if len(factor) != len(target):
        raise ValueError("Factor and target length mismatch")
    mask = np.isfinite(factor) & np.isfinite(target)
    if mask.sum() < 2:
        return 0.0
    f, t = factor[mask], target[mask]
    if np.std(f) == 0 or np.std(t) == 0:
        return 0.0
    return float(np.corrcoef(f, t)[0, 1])


def set_random_seed(seed: int = 42):
    """
    固定全局随机数种子，确保实验可复现。

    Args:
        seed (int): 随机数种子，默认42。
    """
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


================================================================================
File: ./__init__.py
--------------------------------------------------------------------------------


================================================================================
File: ./combination.py
--------------------------------------------------------------------------------
# combination.py
import numpy as np
from utility import zscore_normalize, winsorize, information_coefficient
from scipy.optimize import minimize
from typing import List, Dict, Union, Tuple, Callable
import operators
import pandas as pd

class AlphaCombinationModel:
    """
    因子线性组合管理器，实现论文中算法1的核心思想。

    功能:
      - 维护一个最多包含 `max_pool_size` 条归一化因子序列的因子池。
      - 每当有新因子生成时，执行截尾（Winsorize）和 Z-score 标准化，并计算该因子的单因子 IC（Information Coefficient）。
      - 将新因子加入因子池后，通过凸优化（SLSQP）求解最优线性权重，以最大化组合 IC。
      - 若因子池超过上限，则剔除对组合贡献度最小的因子。
      - 在多次计算中对标准化序列、IC 值、最优权重等中间结果进行缓存，以减少重复开销。

    Attributes:
        max_pool_size (int): 因子池最大容量。超过后将删除贡献最小的因子。
        alphas (List[np.ndarray]): 原始因子序列列表。
        norm_alphas (List[np.ndarray]): 归一化后的因子序列列表。
        ic_list (List[float]): 对应每条因子的单因子 IC 列表。
        weights (List[float]): 当前组合中各因子的线性权重。
        expr_list (List[str]): 保存每条因子对应的 RPN 表达式。
        _cache (Dict): 缓存用于存储中间计算结果。
        data (pd.DataFrame): 注入的行情数据。
        _target (np.ndarray): 注入的目标列（未来收益）数组。
    """
    def __init__(self, max_pool_size: int = 50):
        """
        初始化 AlphaCombinationModel。

        Args:
            max_pool_size (int): 因子池的最大容量，上限内优先保留贡献度高的因子。
        """
        self.max_pool_size = max_pool_size
        self.alphas = []         # 原始因子序列列表
        self.norm_alphas = []    # 归一化后的因子序列列表
        self.ic_list = []        # 对应的单因子 IC 值列表
        self.weights = []        # 当前组合的线性权重列表
        self._cache = {}         # 缓存字典，用于存储中间结果
        self.expr_list = []      # 新增：对应每条因子的 RPN 表达式字符串

    def inject_data(self, df: pd.DataFrame, target_col: str) -> None:
        """
        注入市场行情数据和目标序列，用于 IC 计算与权重优化。

        Args:
            df (pd.DataFrame): 行情特征表，包含基础字段和目标列。
            target_col (str): DataFrame 中代表未来收益的列名，用于 IC 计算。

        Raises:
            ValueError: 当 target_col 不在 df 列时抛出。
        """
        # TODO: 需要更细致的训练集 / 验证集 分割逻辑，添加新的数据集作为验证集 etc
        self.data: pd.DataFrame = df
        self._target = df[target_col].values.astype(np.float64)

    def update_with(self, new_alpha: np.ndarray, expr: str):
        """
        将新因子加入池中并更新组合：
          1. 对原始因子序列做 winsorize 和 z-score 标准化。
          2. 计算并缓存该因子的 IC。
          3. 添加至因子池后，重优化线性权重。
          4. 超出容量时剔除贡献最小的因子。

        Args:
            new_alpha (np.ndarray): 新因子的原始序列数据，长度与注入的行情一致。
            expr (str): 生成该因子的 RPN 表达式字符串，用于记录及缓存键。
        """
        norm = winsorize(zscore_normalize(new_alpha))
        key = ('ic', tuple(norm))
        if key in self._cache:
            ic = self._cache[key]
        else:
            target = self._load_validation_target()
            ic = information_coefficient(norm, target)
            self._cache[key] = ic

        # 更新因子池
        self.alphas.append(new_alpha)
        self.norm_alphas.append(norm)
        self.ic_list.append(ic)
        self.expr_list.append(expr)          # 记录表达式

        # 若是首因子直接赋权 1；否则重优化权重
        if len(self.norm_alphas) == 1:
            self.weights = [1.0]
        else:
            self._reoptimize_weights()

        # 超限时剔除贡献最小因子
        if len(self.alphas) > self.max_pool_size:
            contrib = [abs(w * ic) for w, ic in zip(self.weights, self.ic_list)]
            idx = contrib.index(min(contrib))
            for lst in (self.alphas, self.norm_alphas, self.ic_list, self.weights):
                lst.pop(idx)

    def add_alpha_expr(self, expr: str) -> float:
        """
        根据 RPN 表达式计算新因子，并将其加入因子池。

        Args:
            expr (str): RPN 格式的表达式，例如 "close 5 ts_mean"。

        Returns:
            float: 该因子的单 IC 值，可作为强化学习的 reward。

        Raises:
            ValueError: 当表达式格式错误或运算失败时。
        """
        new_alpha = self._compute_alpha_from_expr(expr)
        ic = self.evaluate_alpha(expr)                  # 里面自带缓存
        self.update_with(new_alpha, expr)
        return ic

    def _reoptimize_weights(self):
        """
        使用凸优化（SLSQP）在当前因子池上求解最优线性权重，
        目标：最大化组合序列与目标的 Pearson 相关系数（IC），
        约束：权重绝对值之和等于 1（L1 归一化）。
        """
        A = np.vstack(self.norm_alphas).T
        target = self._load_validation_target()

        def objective(w):
            combo = A.dot(w)
            ic = np.corrcoef(combo, target)[0, 1]
            return -np.nan_to_num(ic)

        cons = ({'type': 'eq', 'fun': lambda w: np.sum(np.abs(w)) - 1})
        x0 = np.ones(A.shape[1]) / A.shape[1]
        res = minimize(objective, x0, constraints=cons, method='SLSQP')
        self.weights = res.x.tolist()
        self._cache['weights'] = res.x.copy()

    def evaluate_alpha(self, expr: str) -> float:
        """
        直接根据 RPN 表达式计算并返回单因子 IC（带缓存）。
        如果中间任何一步产生了 warning，在缓存完 IC 之后统一抛错。
        """
        new_alpha = self._compute_alpha_from_expr(expr)
        new_alpha = np.nan_to_num(new_alpha, nan=0.0)
        norm = self._maybe_normalize(new_alpha)

        if np.all(np.isnan(new_alpha)):
            return -1.0

        key = ('expr_ic', expr)
        if key not in self._cache:
            target = self._load_validation_target()
            ic = information_coefficient(norm, target)
            self._cache[key] = ic

        return self._cache[key]

    def score(self) -> float:
        """
        计算当前因子组合在验证集上的加权 IC。

        Returns:
            float: 组合因子的 Pearson IC 值。
        """
        A = np.vstack(self.norm_alphas).T
        combo = A.dot(np.array(self.weights))
        target = self._load_validation_target()
        return information_coefficient(combo, target)

    def _load_validation_target(self) -> np.ndarray:
        """
        获取注入的目标序列数组（未来收益或方向）。

        Returns:
            np.ndarray: 目标序列数值数组。

        Raises:
            AttributeError: 若未调用 `inject_data` 注入数据时。
        """
        if not hasattr(self, "_target"):
            raise AttributeError("请先调用 inject_data() 注入行情和目标序列")
        return self._target

    def _compute_alpha_from_expr(self, expr: str) -> np.ndarray:
        """
        解析逆波兰表达式（RPN），执行算子运算，生成原始因子序列。

        Args:
            expr (str): 形如 "close 5 ts_mean" 的 RPN 表达式字符串。

        Returns:
            np.ndarray: 计算得到的因子值数组，dtype=float64。

        Raises:
            AttributeError: 若未注入 `data` 时调用。
            ValueError: 表达式格式错误（未知 token、参数不足或最终栈深 != 1）。
        """
        if not hasattr(self, "data"):
            raise AttributeError(
                "AlphaCombinationModel 需先注入行情 DataFrame 到 self.data"
            )

        tokens: List[str] = expr.strip().split()
        stack: List[Union[pd.Series, float]] = []

        func_map: Dict[str, Tuple[Callable, int]] = operators.FUNC_MAP

        for tk in tokens:
            if tk in self.data.columns:
                stack.append(self.data[tk])
            elif _is_float(tk):
                val = float(tk)
                if val.is_integer():
                    stack.append(int(val))
                else:
                    stack.append(float(tk))
            elif tk in func_map:
                fn, arity, _ = func_map[tk]
                if len(stack) < arity:
                    raise ValueError(f"RPN 表达式参数不足：{tk}")
                args = [stack.pop() for _ in range(arity)][::-1] # 注意：弹栈顺序需反转以保持原来顺序
                res = fn(*args)
                stack.append(res)
            else:
                raise ValueError(f"未知 token：{tk}")

        if len(stack) != 1:
            raise ValueError(f"RPN 表达式最终栈深度应为 1, 计算时为: {len(stack)}")
        output = stack[0]
        if np.isscalar(output):
            series = pd.Series(output, index=self.data.index)
        elif isinstance(output, pd.Series):
            series = output.reindex(self.data.index)
        else:
            series = pd.Series(output, index=self.data.index)

        return series.values.astype(np.float64)
    
    def _maybe_normalize(self, alpha: np.ndarray) -> np.ndarray:
        """
        对原始因子序列执行截尾（winsorize）和 Z-score 标准化。

        Args:
            alpha (np.ndarray): 原始因子值数组。

        Returns:
            np.ndarray: 归一化后的因子序列。
        """
        return winsorize(zscore_normalize(alpha))

        # === 1. RPN 解析与执行 ====================================================

def _is_float(str) -> bool:
    """
    判断字符串是否可转换为浮点数。

    Args:
        s (str): 待检测字符串。

    Returns:
        bool: 若能安全转换为 float，则返回 True，否则 False。
    """
    try:
        float(str)
        return True
    except ValueError:
        return False

================================================================================
File: ./tokenizer.py
--------------------------------------------------------------------------------
# tokenizer.py
import operators
from typing import List, Dict

class AlphaTokenizer:
    """
    将逆波兰表达式 (RPN) 与 token 序列相互映射的分词器。

    - **词表**：基础行情字段、常量桶、所有算子以及四则运算符  
    - **特殊标记**：`[PAD]` 填充、`[BOS]` 序列起始、`[SEP]` 序列终止
    """
    def __init__(self, base_fields: List[str] = None, const_buckets: List[float] = None):
        """
        构造分词器并自动扫描 `operators.py` 生成完整词表。

        参数
        ----------
        base_fields : list[str], 可选
            基础行情字段名称列表，默认为  
            ``['open', 'high', 'low', 'close', 'volume']``。
        const_buckets : list[float], 可选
            预定义浮点常量桶，默认为  
            ``[1, 3, 5, 10, 20, 0.1, 0.5]``。
        """
        if base_fields is None:
            base_fields = ['open', 'high', 'low', 'close', 'volume']
        if const_buckets is None:
            const_buckets = [1, 3, 5, 10, 20, 0.1, 0.5]

        self.base_fields = base_fields
        self.const_buckets = const_buckets

        self.special_tokens = ['[PAD]', '[BOS]', '[SEP]']
        field_tokens = base_fields
        const_tokens = [f'CONST_{c}' for c in const_buckets]
        op_tokens: List = list(operators.FUNC_MAP.keys())
        
        self.vocab: List[str] = self.special_tokens + field_tokens + const_tokens + op_tokens

        self.token_to_id: Dict[str, int] = {tok: idx for idx, tok in enumerate(self.vocab)}
        self.id_to_token: Dict[int, str] = {idx: tok for tok, idx in self.token_to_id.items()}
        self.operand_type_map: dict[str, str] = {}

        for f in self.base_fields:
            self.operand_type_map[f] = "Series"
        for c in self.const_buckets:
            tok = f"CONST_{c}"
            # 先把 c 转成 float，再看是不是整数
            if float(c).is_integer():
                self.operand_type_map[tok] = "Scalar_INT"
            else:
                self.operand_type_map[tok] = "Scalar_FLOAT"

        self.pad_token_id = self.token_to_id['[PAD]']
        self.bos_token_id = self.token_to_id['[BOS]']
        self.sep_token_id = self.token_to_id['[SEP]']

    @property
    def vocab_size(self) -> int:
        """
        返回当前词表大小。

        返回
        ----------
        int
            ``len(self.vocab)``。
        """
        return len(self.vocab)

    def encode(self, expr: str, add_special_tokens: bool = True) -> List[int]:
        """
        将 RPN 字符串编码为 token ID 序列。

        参数
        ----------
        expr : str
            形如 ``"close 5 ts_mean"`` 的逆波兰表达式。
        add_special_tokens : bool, 可选
            是否在首尾分别加入 `[BOS]` 与 `[SEP]`，默认为 ``True``。

        返回
        ----------
        list[int]
            token ID 序列。
        """
        tokens = expr.strip().split()
        ids = []
        for tk in tokens:
            # 基础字段
            if tk in self.token_to_id:
                ids.append(self.token_to_id[tk])
            # 常量：动态加入最近的 bucket
            elif self._is_float(tk):
                val = float(tk)
                const_tok = self._map_to_const(val)
                ids.append(self.token_to_id[const_tok])
            else:
                raise ValueError(f"未知 token：{tk}")

        if add_special_tokens:
            ids = [self.bos_token_id] + ids + [self.sep_token_id]
        return ids

    def decode(self, ids: List[int], remove_special_tokens: bool = True) -> str:
        """
        将 token ID 序列还原为 RPN 字符串。

        参数
        ----------
        ids : list[int]
            编码后的 token ID 列表。
        remove_special_tokens : bool, 可选
            是否去除 `[BOS]` / `[SEP]` / `[PAD]`，默认为 ``True``。

        返回
        ----------
        str
            逆波兰表达式，常量会被还原为原始数字字符串。
        """
        toks = []
        for idx in ids:
            if remove_special_tokens and idx in [self.bos_token_id, self.sep_token_id, self.pad_token_id]:
                continue
            toks.append(self.id_to_token.get(idx, 'UNK'))
        toks = [self._const_to_str(tk) for tk in toks]
        return " ".join(toks)

    def _map_to_const(self, val: float) -> str:
        """
        将任意浮点数映射到最近的常量桶，并返回其 token 名。

        参数
        ----------
        val : float
            原始浮点数。

        返回
        ----------
        str
            形如 ``"CONST_5"`` 的 token 名称。
        """
        closest = min(self.const_buckets, key=lambda x: abs(x - val))
        return f'CONST_{closest}'

    def _const_to_str(self, token: str) -> str:
        """
        将常量 token 名恢复为数字字符串。

        参数
        ----------
        token : str
            形如 ``"CONST_3"`` 的 token。

        返回
        ----------
        str
            ``"3"``；如果传入非常量 token，则原样返回。
        """
        if token.startswith('CONST_'):
            return token.split('_', 1)[1]
        return token

    def rpn_to_infix(self, rpn: str) -> str:
        """
        将逆波兰表达式 (RPN) 转换为可读的中缀表达式。

        参数
        ----------
        rpn : str
            形如 `"close 5 ts_mean high low sub mul"` 的 RPN 式字符串。

        返回
        ----------
        str
            对应的中缀表达式字符串，例如 `"(ts_mean(close, 5) * (high - low))"`。
        """
        from operators import FUNC_MAP

        stack: List[str] = []
        tokens = rpn.strip().split()
        for tk in tokens:
            # 操作符
            if tk in FUNC_MAP:
                fn, arity, _ = FUNC_MAP[tk]
                if arity == 1:
                    a = stack.pop()
                    stack.append(f"{tk}({a})")
                elif arity == 2:
                    b = stack.pop()
                    a = stack.pop()
                    if tk == "add":
                        stack.append(f"({a} + {b})")
                    elif tk == "sub":
                        stack.append(f"({a} - {b})")
                    elif tk == "mul":
                        stack.append(f"({a} * {b})")
                    elif tk == "div":
                        stack.append(f"({a} / {b})")
                    else:
                        stack.append(f"{tk}({a}, {b})")
                else:
                    args = ", ".join(stack[-arity:])
                    stack = stack[:-arity]
                    stack.append(f"{tk}({args})")
            else:
                val = self._const_to_str(tk)
                stack.append(val)
        return stack[0] if stack else ""

    @staticmethod
    def _is_float(s: str) -> bool:
        """
        判断字符串能否安全转换为 `float`。

        参数
        ----------
        s : str
            待检测字符串。

        返回
        ----------
        bool
            可转换返回 ``True``，否则 ``False``。
        """
        try:
            float(s)
            return True
        except ValueError:
            return False



================================================================================
File: ./generator.py
--------------------------------------------------------------------------------
# generator.py
from typing import List, Tuple, Dict, Any
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from torch.distributions import Categorical
from alpha_generation_env import AlphaGenerationEnv


class PolicyNetwork(nn.Module):
    def __init__(self, vocab_size: int, hidden_dim: int, num_layers: int = 1) -> None:
        """
        策略网络，用于生成下一个 token 的概率分布。

        Args:
            vocab_size: token vocabulary 大小，即动作空间大小。
            hidden_dim: LSTM 隐藏层维度。
            num_layers: LSTM 层数。
        """
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)
        self.fc_logits = nn.Linear(hidden_dim, vocab_size)

    def forward(
        self, x: torch.Tensor, hidden: Tuple[torch.Tensor, torch.Tensor]
    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        Args:
            x: 输入 token 序列，形状为 (batch_size, seq_len)。
            hidden: LSTM 的隐藏状态 (h, c)，形状为 (num_layers, batch_size, hidden_dim)。

        Returns:
            logits: 下一个 token 的 logits，形状为 (batch_size, vocab_size)。
            hidden: 更新后的隐藏状态。
        """
        emb = self.embedding(x)
        out, hidden = self.lstm(emb, hidden)
        logits = self.fc_logits(out[:, -1, :])
        return logits, hidden

    def init_hidden(
        self, batch_size: int, device: torch.device
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        初始化隐藏状态为全零。

        Args:
            batch_size: 批量大小。
            device: 所在设备（cpu 或 cuda）。

        Returns:
            初始化的 (h, c) 状态。
        """
        num_layers = self.lstm.num_layers
        hidden_dim = self.lstm.hidden_size
        h0 = torch.zeros(num_layers, batch_size, hidden_dim, device=device)
        c0 = torch.zeros(num_layers, batch_size, hidden_dim, device=device)
        return (h0, c0)


class ValueNetwork(nn.Module):
    def __init__(self, vocab_size: int, hidden_dim: int, num_layers: int = 1) -> None:
        """
        价值网络，用于估计当前 token 序列的状态价值 V(s)

        Args:
            vocab_size: token vocabulary 大小。
            hidden_dim: LSTM 隐藏层维度。
            num_layers: LSTM 层数。
        """
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)
        self.fc_value = nn.Linear(hidden_dim, 1)

    def forward(
        self, x: torch.Tensor, hidden: Tuple[torch.Tensor, torch.Tensor]
    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        参数:
            x: 输入 token 序列，形状为 (batch_size, seq_len)。
            hidden: LSTM 隐藏状态。

        返回值:
            value: 每个序列的状态价值，形状为 (batch_size,)。
            hidden: 更新后的隐藏状态。
        """
        emb = self.embedding(x)
        out, hidden = self.lstm(emb, hidden)
        value = self.fc_value(out[:, -1, :]).squeeze(-1)
        return value, hidden

    def init_hidden(
        self, batch_size: int, device: torch.device
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        同 PolicyNetwork，初始化 LSTM 隐藏状态。
        """
        num_layers = self.lstm.num_layers
        hidden_dim = self.lstm.hidden_size
        h0 = torch.zeros(num_layers, batch_size, hidden_dim, device=device)
        c0 = torch.zeros(num_layers, batch_size, hidden_dim, device=device)
        return (h0, c0)


class RLAlphaGenerator:
    """
    使用 PPO 在 `AlphaGenerationEnv` 中训练策略网络，自动生成高 IC 的 Alpha 表达式。
    """

    def __init__(self, env: AlphaGenerationEnv, config: Dict[str, Any]) -> None:
        """
        参数:
            env: 强化学习环境，需支持 reset(), step(action), valid_actions() 接口。
            config: 包含网络和 PPO 超参数的配置字典。
        """
        self.env: AlphaGenerationEnv = env
        self.vocab_size = config["vocab_size"]
        self.hidden_dim = config.get("hidden_dim", 128)
        self.device = config.get("device", "cpu")

        self.policy_net = PolicyNetwork(self.vocab_size, self.hidden_dim).to(
            self.device
        )
        self.value_net = ValueNetwork(self.vocab_size, self.hidden_dim).to(self.device)

        self.policy_optimizer = torch.optim.AdamW(
            self.policy_net.parameters(), lr=config.get("lr_policy", 3e-4)
        )
        self.value_optimizer = torch.optim.AdamW(
            self.value_net.parameters(), lr=config.get("lr_value", 1e-3)
        )

        self.gamma = config.get("gamma", 1.0)
        self.clip_eps = config.get("clip_eps", 0.2)
        self.entropy_coef = config.get("entropy_coef", 0.01)
        self.value_coef = config.get("value_coef", 0.5)
        self.update_epochs = config.get("update_epochs", 4)
        self.batch_size = config.get("batch_size", 64)
        self.max_seq_len = config.get("max_seq_len", 20)
    
    def train(self, num_iterations: int) -> None:
        """
        用 PPO 训练策略 & 价值网络。
        每轮：
          ① 与环境交互，采样 batch_size 步（或更多）完整 episode
          ② 计算优势 (A = R - V) 并标准化
          ③ 对策略 / 价值网络做多次 epoch 更新
        """
        for it in range(1, num_iterations + 1):
            # ------ 采样轨迹 -------------------------------------------------
            states, actions, old_logps, returns, advantages = self._collect_trajectories()

            # Advantage 标准化以稳定训练
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

            # 打包成 DataLoader，方便多轮 epoch shuffle
            ds = TensorDataset(states, actions, old_logps, returns, advantages)
            loader = DataLoader(ds, batch_size=self.batch_size, shuffle=True)

            for _ in range(self.update_epochs):
                for s, a, logp_old, ret, adv in loader:
                    s = s.to(self.device)
                    a = a.to(self.device)
                    logp_old = logp_old.to(self.device).detach()
                    ret = ret.to(self.device)
                    adv = adv.to(self.device)

                    # ----- 1. 重新计算策略 & log π(a|s) ---------------------
                    h0_p = self.policy_net.init_hidden(s.size(0), self.device)
                    logits, _ = self.policy_net(s, h0_p)
                    dist = Categorical(logits=logits)
                    logp = dist.log_prob(a)
                    entropy = dist.entropy().mean()

                    # ----- 2. 计算 PPO clip 损失 ---------------------------
                    ratio = (logp - logp_old).exp()
                    pg_loss = -torch.min(
                        ratio * adv,
                        torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * adv,
                    ).mean()

                    # ----- 3. 计算价值函数损失 -----------------------------
                    h0_v = self.value_net.init_hidden(s.size(0), self.device)
                    value_pred, _ = self.value_net(s, h0_v)
                    value_loss = F.mse_loss(value_pred.squeeze(-1), ret)

                    # ----- 4. 总损失 & 反向传播 ----------------------------
                    loss = pg_loss + self.value_coef * value_loss - self.entropy_coef * entropy

                    self.policy_optimizer.zero_grad()
                    self.value_optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
                    torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 1.0)
                    self.policy_optimizer.step()
                    self.value_optimizer.step()

            # ------ 打印监控信息 --------------------------------------------
            if it % 10 == 0:
                with torch.no_grad():
                    avg_ret = returns.mean().item()
                    combo_ic = self.env.combo_model.score()
                print(f"[Iter {it:04d}]  AvgReturn={avg_ret:+.4f}   ComboIC={combo_ic:+.4f}")

    def _collect_trajectories(
        self,
    ) -> Tuple[
        List[torch.Tensor], List[int], List[torch.Tensor], List[float], List[float]
    ]:
        """
        从环境中采样一批完整轨迹，并计算回报与优势值（GAE λ=1）。

        返回
        ----------
        states : torch.Tensor
            形状 ``(T, seq_len)`` 的 token 序列张量。
        actions : torch.Tensor
            长度 ``T`` 的动作 ID。
        logps : torch.Tensor
            对应动作的对数概率。
        returns : torch.Tensor
            折现回报。
        advantages : torch.Tensor
            Advantage 值（此处为 `return - value`）。
        """
        states, actions, logps, rewards, dones, values = [], [], [], [], [], []

        obs = torch.tensor([self.env.reset()], device=self.device)
        h_p, h_v = self.policy_net.init_hidden(1, self.device), self.value_net.init_hidden(1, self.device)

        while len(states) < self.batch_size:
            logits, h_p = self.policy_net(obs, h_p)

            # -------- Invalid-action-mask --------
            valid = self.env.valid_actions()
            if len(valid) == 0:
                raise RuntimeError("没有合法动作")
            else:
                mask = torch.full((self.vocab_size,), float('-inf'), device=self.device)
                mask[valid] = 0.0                                   # 合法动作设置成 0，其它仍为 −inf
                logits = logits + mask                              # 非法动作 logits 变为 −inf
                dist = Categorical(logits=logits)
                action = dist.sample()
                logp = dist.log_prob(action)

                value, h_v = self.value_net(obs, h_v)
                value = value.squeeze(0).detach()
            
                next_obs, reward, done, _ = self.env.step(action.item())
            
            pad_id = self.env.tokenizer.pad_token_id
            raw = obs.squeeze(0)  # 长度不定
            pad = torch.full((self.max_seq_len,), pad_id, dtype=torch.long, device=self.device)
            if raw.size(0) >= self.max_seq_len:
                pad[:] = raw[-self.max_seq_len:]
            else:
                pad[-raw.size(0):] = raw
            
            states.append(pad)
            actions.append(action)
            logps.append(logp)
            rewards.append(torch.tensor(reward, device=self.device, dtype=torch.float32))
            dones.append(done)
            values.append(value)

            if done:
                obs = torch.tensor([self.env.reset()], device=self.device)
                h_p, h_v = self.policy_net.init_hidden(1, self.device), self.value_net.init_hidden(1, self.device)
            else:
                obs = torch.tensor([next_obs], device=self.device)
        
        # ===== 计算 GAE(λ=1) → advantage = return - value =====
        returns, advantages, R = [], [], torch.tensor(0.0, device=self.device)
        for r, v, d in zip(reversed(rewards), reversed(values), reversed(dones)):
            R = r + self.gamma * R * (1.0 - float(d))
            returns.insert(0, R)
            advantages.insert(0, R - v)
        
        return (
            torch.stack(states),
            torch.stack(actions).squeeze(-1),
            torch.stack(logps),
            torch.stack(returns),
            torch.stack(advantages),
        )
    
    

================================================================================
File: ./paradigm.py
--------------------------------------------------------------------------------
import numpy as np

# alpha_mining_framework/
# ├── __init__.py
# ├── data.py            # Data loading & preprocessing
# ├── utils.py           # Metrics: IC, RankIC, normalization, caching
# ├── combination.py     # AlphaCombinationModel: linear combine + weight optimization
# ├── env.py             # AlphaGenerationEnv: custom RL environment without gym
# ├── generator.py       # RLAlphaGenerator: PPO policy & value networks
# ├── train.py           # Main training loop implementing Algorithm 2
# └── run_backtest.py    # Backtesting & investment simulation (Figure 5)






================================================================================
File: ./train.py
--------------------------------------------------------------------------------
# train.py
import os
import yaml
import pandas as pd

from utility import set_random_seed
from data import load_market_data
from combination import AlphaCombinationModel
from tokenizer import AlphaTokenizer
from alpha_generation_env import AlphaGenerationEnv
from generator import RLAlphaGenerator


def main(config_path: str = "config.yaml"):
    with open(config_path, "r") as f:
        cfg = yaml.safe_load(f)

    set_random_seed(cfg.get("random_seed", 42))

    data_cfg = cfg["data"]
    df = load_market_data(
        path=data_cfg["path"], multiplier=data_cfg["multiplier"], n=data_cfg["n"]
    )

    combo = AlphaCombinationModel(max_pool_size=cfg["model"]["max_pool_size"])
    combo.inject_data(df, target_col=data_cfg["target_col"])

    tokenizer = AlphaTokenizer()
    env = AlphaGenerationEnv(
        combo_model=combo, tokenizer=tokenizer, max_len=cfg["env"]["max_len"]
    )

    gen_cfg = cfg["generator"]
    gen_cfg["vocab_size"] = tokenizer.vocab_size
    gen_cfg["max_seq_len"] = cfg["env"]["max_len"]
    agent = RLAlphaGenerator(env=env, config=gen_cfg)

    print(f"Starting training for {gen_cfg['num_iterations']} iterations...")
    agent.train(num_iterations=gen_cfg["num_iterations"])

    out_cfg = cfg["output"]
    os.makedirs(os.path.dirname(out_cfg["alphas_weights_path"]), exist_ok=True)
    results = pd.DataFrame(
        {"expr": combo.expr_list, "ic": combo.ic_list, "weight": combo.weights}
    )
    results.to_csv(out_cfg["alphas_weights_path"], index=False)
    print(f"Saved discovered alphas and weights to {out_cfg['alphas_weights_path']}")


if __name__ == "__main__":
    main()


================================================================================
File: ./operators.py
--------------------------------------------------------------------------------
# operators.py
import pandas as pd
import numpy as np
import inspect
from typing import List, Union

# -----------------------------
# Basic Operators
# -----------------------------

ARITH_OPS = {'add', 'sub', 'mul', 'div',}

ScalarOrSeries = Union[pd.Series, float, int]

def add(x: ScalarOrSeries, y: ScalarOrSeries) -> ScalarOrSeries:
    return x + y

def sub(x: ScalarOrSeries, y: ScalarOrSeries) -> ScalarOrSeries:
    return x - y

def mul(x: ScalarOrSeries, y: ScalarOrSeries) -> ScalarOrSeries:
    return x * y

def div(x: ScalarOrSeries, y: ScalarOrSeries) -> ScalarOrSeries:
    if isinstance(y, pd.Series):
        y = y.replace(0, np.nan)
    elif y == 0:
        y = np.nan
    return x / y

# -----------------------------
# Unary Operators
# -----------------------------

def abs_(x: pd.Series) -> pd.Series:
    return x.abs()

def sign(x: pd.Series) -> pd.Series:
    return np.sign(x)

def signed_log(x: pd.Series) -> pd.Series:
    a = x.abs().replace(0, np.nan)
    lg = np.log(a)
    return np.sign(x) * lg

def signed_sqrt(x: pd.Series) -> pd.Series:
    return np.sign(x) * np.sqrt(x.abs())

def neg(x: pd.Series) -> pd.Series:
    return -x


# -----------------------------
# Time-Series Operators (TS)
# -----------------------------

def ref(series: pd.Series, window: int) -> pd.Series:
    """Ref(x, t)：滞后期 t 的值"""
    return series.shift(window)

def ts_mean(series: pd.Series, window: int) -> pd.Series:
    """Mean(x, t)：滚动均值"""
    return series.rolling(window).mean()

def ts_med(series: pd.Series, window: int) -> pd.Series:
    """Med(x, t)：滚动中位数"""
    return series.rolling(window).median()

def ts_sum(series: pd.Series, window: int) -> pd.Series:
    """Sum(x, t)：滚动求和"""
    return series.rolling(window).sum()

def ts_std(series: pd.Series, window: int) -> pd.Series:
    """Std(x, t)：滚动标准差"""
    return series.rolling(window).std()

def ts_var(series: pd.Series, window: int) -> pd.Series:
    """Var(x, t)：滚动方差"""
    return series.rolling(window).var()

def ts_skew(series: pd.Series, window: int) -> pd.Series:
    """滚动偏度"""
    return series.rolling(window).skew()

def ts_kurt(series: pd.Series, window: int) -> pd.Series:
    """滚动峰度"""
    return series.rolling(window).kurt()

def ts_max(series: pd.Series, window: int) -> pd.Series:
    """Max(x, t)：滚动最大值"""
    return series.rolling(window).max()

def ts_min(series: pd.Series, window: int) -> pd.Series:
    """Min(x, t)：滚动最小值"""
    return series.rolling(window).min()

def ts_mad(series: pd.Series, window: int) -> pd.Series:
    """Mad(x, t)：滚动平均绝对偏差"""
    return series.rolling(window).apply(
        lambda x: np.mean(np.abs(x - np.mean(x))), raw=True
    )

def ts_delta(series: pd.Series, window: int = 1) -> pd.Series:
    """Delta(x, t)：与 t 期前的差值"""
    return series.diff(window)

def ts_rank(series: pd.Series, window: int) -> pd.Series:
    """滚动排序百分位"""
    def _rank(x):
        return pd.Series(x).rank(pct=True).iloc[-1]
    return series.rolling(window).apply(_rank, raw=True)

def ts_corr(x: pd.Series, y: pd.Series, window: int) -> pd.Series:
    """Corr(x, y, t)：滚动皮尔森相关系数"""
    return x.rolling(window).corr(y)

def ts_cov(x: pd.Series, y: pd.Series, window: int) -> pd.Series:
    """Cov(x, y, t)：滚动协方差"""
    return x.rolling(window).cov(y)

def ts_wma(series: pd.Series, window: int) -> pd.Series:
    """WMA(x, t)：滚动线性加权平均"""
    weights = np.arange(1, window + 1)
    return series.rolling(window).apply(
        lambda x: np.dot(x, weights) / weights.sum(), raw=True
    )

def ts_ema(series: pd.Series, window: int) -> pd.Series:
    """EMA(x, t)：指数移动平均（span 可理解为 t）"""
    return series.ewm(span=window, adjust=False).mean()


# -----------------------------
# Utility Operators
# -----------------------------

def decay_linear(series: pd.Series, window: int) -> pd.Series:
    """线性衰减加权平均（同 WMA）"""
    weights = np.arange(1, window + 1)[::-1]
    return series.rolling(window).apply(
        lambda x: np.dot(x, weights) / weights.sum(), raw=True
    )

def ts_zscore(series: pd.Series, window: int) -> pd.Series:
    """时序标准分"""
    return (series - ts_mean(series, window)) / ts_std(series, window)

def ts_return(series: pd.Series, window: int = 1) -> pd.Series:
    """周期收益率"""
    return series.pct_change(window, fill_method=None)


FUNC_MAP: dict[str, tuple[callable, int, List[str]]] = {}

for name, fn in inspect.getmembers(__import__(__name__), inspect.isfunction):
    if name.startswith("_"):
        continue

    if name in ARITH_OPS:
        FUNC_MAP[name] = (fn, 2, ['Any', 'Any'])  # 'Any' 表示 Scalar 或 Series 都接受
        continue

    sig = inspect.signature(fn)
    param_types = []
    for param in sig.parameters.values():
        pname = param.name
        if pname in ('x', 'y', 'series'):
            param_types.append('Series')
        elif pname in ('window', 'n'):
            param_types.append('Scalar_INT')
        else:
            raise RuntimeError("Unrecognizable param type.")
    FUNC_MAP[name] = (fn, len(param_types), param_types)



================================================================================
File: ./data.py
--------------------------------------------------------------------------------
# data.py
# TODO: 逻辑需要更加细化，
# 1. 过滤不合格的交易时间--针对不同的交易品种 
# 2. 怎么样弹性地满足不同跨度因子的需求，整理成 numpy.array 高效地计算 ic，现在处理方式为 直接concat，很不合理
#   a. 不同的时间窗口; 
#   b. 不同的 symbol;


from typing import List, Dict
from datetime import datetime
from pathlib import Path
import pandas as pd
import numpy as np
from joblib import Parallel, delayed

def load_and_process(file: Path, multiplier: int, n: int) -> pd.DataFrame:
    df = pd.read_parquet(file)
    return process_tick_data(df, multiplier=multiplier, n=n)

def load_symbol_dfs(
    directory: str,
    symbols: Dict[str, int],
    start_date: str,
    end_date: str,
    n_jobs: int = 4,
    n: int = 10
) -> Dict[str, List[pd.DataFrame]]:
    dir_path = Path(directory)
    dt_start = datetime.strptime(start_date, "%Y%m%d").date()
    dt_end   = datetime.strptime(end_date,   "%Y%m%d").date()

    symbol_dfs: Dict[str, List[pd.DataFrame]] = {}

    for sym, mul in symbols.items():
        files = []
        for file in dir_path.glob(f"{sym}_*.parquet"):
            date_str = file.stem.split("_", 1)[1]
            try:
                file_date = datetime.strptime(date_str, "%Y%m%d").date()
            except ValueError:
                continue
            if dt_start <= file_date <= dt_end:
                files.append(file)
        files = sorted(files)

        processed_list = Parallel(n_jobs=n_jobs, backend="loky")(
            delayed(load_and_process)(file, mul, n)
            for file in files
        )
        symbol_dfs[sym] = processed_list

    return symbol_dfs

def process_tick_data(df: pd.DataFrame, multiplier: int = 10, n: int = 5):

    df = df.copy()
    df = df.set_index("timestamp").sort_index()

    df["d_vol"] = df["volume"].diff()
    df["d_amt"] = df["amount"].diff()
    df["d_oi"] = df["openInterest"].diff()

    df.loc[df["d_vol"] <= 0, ["d_vol", "d_amt"]] = np.nan

    df["trade_price"] = df["d_amt"] / df["d_vol"] / multiplier

    df['ts_ceil'] = df.index.to_series().dt.ceil('s')

    group = df.groupby('ts_ceil', sort=True)
    ohlc = pd.DataFrame({
        'open':         group['last'].first(),
        'high':         group['trade_price'].max(),
        'low':          group['trade_price'].min(),
        'close':        group['last'].last(),
        'volume':       group['d_vol'].sum(),
        'amount':       group['d_amt'].sum(),
        'openInterest': group['d_oi'].sum(),
    })

    ohlc.index.name = 'timestamp'

    mask = ohlc['high'].isna()
    if mask.any():
        last_max = group['last'].max()
        last_min = group['last'].min()
        ohlc.loc[mask, 'high'] = last_max[mask]
        ohlc.loc[mask, 'low']  = last_min[mask]

    ohlc["target"] = ohlc["close"].pct_change(periods=-n, fill_method=None)
    return ohlc




